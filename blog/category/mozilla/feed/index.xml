<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Gregory Szorc's Digital Home</title>
    <link>http://gregoryszorc.com/blog</link>
    <description>Rambling on</description>
    <pubDate>Tue, 13 Jan 2015 23:57:13 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Major bzexport Updates</title>
      <link>http://gregoryszorc.com/blog/2015/01/13/major-bzexport-updates</link>
      <pubDate>Tue, 13 Jan 2015 15:55:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/13/major-bzexport-updates</guid>
      <description>Major bzexport Updates</description>
      <content:encoded><![CDATA[<p>The <em>bzexport</em> Mercurial extension - an extension that enables you to
easily create new Bugzilla bugs and upload patches to Bugzilla for
review - just received some
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1033394">major updates</a>.</p>
<p>First, we now have automated test coverage of bzexport! This is built on
top of the version control test harness I
<a href="/blog/2014/10/14/robustly-testing-version-control-at-mozilla/">previously blogged about</a>.
As part of the tests, we start Docker containers that run the same code
that's running on <a href="https://bugzilla.mozilla.org/">bugzilla.mozilla.org</a>,
so interactions with Bugzilla are properly tested. This is much, much
better than mocking HTTP requests and responses because if Bugzilla
changes, our tests will detect it. Yay continuous integration.</p>
<p>Second, bzexport now uses Bugzilla' REST API instead of the legacy bzAPI
endpoint for all but 1 HTTP request. This should make BMO maintainers
very happy.</p>
<p>Third and finally, bzexport now uses shared code for obtaining Bugzilla
credentials. The behavior is
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmozilla/auth.html">documented</a>,
of course. Behavior is <strong>not backwards compatible</strong>. If you were using some
old configuration values, you will now see warnings when running bzexport.
These warnings are actionable, so I shouldn't need to describe them
here.</p>
<p>Please obtain the new code by pulling the
<a href="https://hg.mozilla.org/hgcustom/version-control-tools">version-control-tools</a>
repository. Or, if you have a Firefox clone, run <em>mach mercurial-setup</em>.</p>
<p>If you find any regressions, file a bug in the
<a href="https://bugzilla.mozilla.org/enter_bug.cgi?product=Developer%20Services&amp;component=Mercurial%3A%20bzexport">Developers Services :: Mercurial: bzexport</a>
component and have it depend on
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1033394">bug 1033394</a>.</p>
<p>Thanks go out to Steve Fink, Ed Morley, and Ted Mielczarek for looking
at the code.</p>]]></content:encoded>
    </item>
    <item>
      <title>Utilizing GitHub for Firefox Development</title>
      <link>http://gregoryszorc.com/blog/2015/01/12/utilizing-github-for-firefox-development</link>
      <pubDate>Mon, 12 Jan 2015 11:00:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/12/utilizing-github-for-firefox-development</guid>
      <description>Utilizing GitHub for Firefox Development</description>
      <content:encoded><![CDATA[<p>Recent posts on my blog have talked about the
<a href="/blog/2015/01/09/firefox-contribution-process-debt/">difficulty submitting changes to Firefox</a>
and the <a href="/blog/2015/01/10/code-first-and-the-rise-of-the-dvcs-and-github/">rise of GitHub</a>.
I encourage you to stop reading this post and read them now if you
haven't already.</p>
<p>As I was looking at the
<a href="/blog/2015/01/09/firefox-contribution-process-debt/#list-of-process-debt">list of process debt contributing to Firefox</a>,
one thought kept creeping into my mind: <strong>how many of these items go
away if we utilize GitHub?</strong></p>
<p>As I mentioned in these two posts, GitHub's popularity has essentially
commoditized many items on this list, especially the parts around source
control and submitting patches for consideration (just fork and open a
pull request). It seems that everyone these days is on GitHub and asking
people to use GitHub to send changes to Firefox would almost certainly
be well-received by contributors and even Mozilla staff.</p>
<p>Here's what I think: <strong>Mozilla should utilize GitHub for Firefox
development.</strong></p>
<p>The verb in that sentence is important: I purposefully said <em>utilize</em>
and not something like <em>switch to</em>. <strong>To switch or not to switch to
GitHub for Firefox development is a false dillemma and a logical
fallacy.</strong> So is the question about <em>switching to Git</em>. As I explain later,
there is a spectrum of options available and <em>switching</em> or <em>not
switching</em> are on the extremes. <em>Utilize</em> doesn't preclude a binary
<em>switch</em> or <em>don't switch</em> outcome, but it does keep an array of options
on the table for consideration.</p>
<p>So, how should Mozilla <em>utilize</em> GitHub for Firefox development?</p>
<p>I think that insisting people establish Bugzilla accounts and upload
patches to Bugzilla/bugs is an anitquated practice in desperate need of
an overhaul. I think that if someone has written code, they should be
able to essentially throw it over a wall to initiate the change process.
They should be able to do this in a manner that incurs little to no
<em>process debt</em>. We, Mozilla, should be able to take only code and
integrate it into Firefox, assuming a trusted person - a module owner
or peer - agrees and grants review. <strong>GitHub pull requests would
facilitate a lesser-involved code contribution mechanism.</strong></p>
<p>Another benefit of GitHub is that the web interface goes further than
just code submission: they also have facilities for editing files. It's
possible to
<a href="https://help.github.com/articles/editing-files-in-another-user-s-repository/">edit a file in someone else's repository and create a pull request</a>
direct from the web interface! My
<a href="/blog/2015/01/09/firefox-contribution-process-debt/">post on process debt</a>
began by comparing the process of <em>edit a wiki</em> versus the current
Firefox change process. GitHub's web-based editing essentially reduces
the gap to cosmetic differences. <strong>GitHub's ease of contributing purely
via the browser would open the door to more contribution for
lesser-involved changes</strong> (sometimes referred to as <em>good first bugs</em>).</p>
<p>To state it explicitly, <strong>I support the use of GitHub pull requests for
submitting changes to Firefox</strong>.</p>
<p>Now, there are some concerns and challenges about doing this. These
include:</p>
<ul>
<li>Fragmentation of code review and tracking could be problematic for
  Mozilla staff and other highly-active individuals.</li>
<li>GitHub can lose some parts of code review after rebasing and force
  pushes. <strong>Edit: Comments below indicate this is no longer a problem.
  Great!</strong></li>
<li>You can only assign 1 reviewer per pull request.</li>
<li>GitHub sends an email/notification per review comment. This can be
  extremely annoying for some mail clients.</li>
<li>GitHub doesn't have a mechanism for dealing with security bugs.</li>
<li>Data sovereignty concerns (all data hosted on GitHub and subject to
  their data retention and access policies). Their API has query limits,
  which can limit machine use somewhat.</li>
<li>GitHub's model favors merges over rebases. Merges have a number of
  downsides, especially for large projects, and we strongly prefer to
  maintain our mostly-linear Firefox repository history.</li>
<li>GitHub's model favors appending commits rather than rewriting commits.
  (This is due to Git badness when you force push.) Mozilla favors a
  world where the final commit is what's reviewed and landed.</li>
<li>Git != Mercurial. Firefox is canonically stored in Mercurial. There is
  some impedence mismatch here. But nothing tools can't overcome.</li>
<li>The <em>Merge Pull Request</em> button is almost completely useless for
  Firefox's existing and future workflows. This partially invalidates
  other niceness the pure GitHub pull request workflow buys you.</li>
<li>Everything is lumped into a single bucket. We lose component-level
  subscriptions, making following harder.</li>
<li>Following the entire Firefox project on GitHub would produce an
  overwhelming fire hose of data.</li>
<li>We don't control GitHub and our options for extending it to extract
  even more process optimization are limited to what their APIs support
  and what they choose to implement.</li>
<li>We are at the whim of GitHub should they ever change a feature or API.</li>
<li>See also <a href="https://github.com/servo/servo/wiki/Github-challenges">Servo's list of challenges</a>.</li>
</ul>
<p>Some of these issues can be overcome by tools and automation (which I
would happily build in my capacity as a Developer Productivity Engineer
at Mozilla). Others are more fundamental and seemingly would require
buy-in and/or support from very senior Mozillians.</p>
<p>If Mozilla were to go forward utilizing GitHub pull requests for
Firefox, I think it should be done incrementally rather than going
all-in and attempting the entire GitHub workflow from the start.
Although, this would mean diverging from GitHub's well-known practices,
which would <em>increase</em> process debt from the GitHub base level. I don't
like that. But I think it is a step in the right direction. Partial
reduction in process debt is better than no reduction.</p>
<p>What do I mean by <em>incrementally</em> start accepting pull requests? Well,
<strong>I don't think code review should initially be conducted on GitHub</strong>.
When you look at the above list of concerns, many of them are around code
review and interacting with pull requests. I think there's too much
badness and risk there to make me comfortable about <em>moving</em> things to
GitHub and giving GitHub exclusive domain over this important data,
at least initially.</p>
<p>But if code review isn't conducted on GitHub, what's the value of a pull
request? <strong>A pull request would be a well-defined and well-understood
mechanism for importing data into Mozilla's systems</strong>. For example,
submitting a pull request would automatically result in the creation
of a review request on
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/mozreview.html">MozReview</a>
or even a bug/attachment/review on Bugzilla.
This would allow people to send code to Mozilla easily while
simultaneously allowing Mozillians to use familiar tools and processes
without the aforementioned concerns with GitHub. That appears to be
win-win.</p>
<p>Once we have a simple mechanism in place for turning pull requests into
MozReview's review requests, we can start playing around with the
syncing of code review activity between Mozilla and GitHub so review
activity on either system results in cross-posting. There is precedent
for this today. <a href="http://gerrithub.io/">GerritHub</a> has bi-directional
syncing of code review activity between GitHub and Gerrit. Facebook also
does something similar, syncing data between their internal Phabricator
instance. Mozilla to GitHub sync would not be difficult: we control all
those systems and I'm pretty confident in our ability to make a GitHub API
call when a MozReview review request is updated (we already make
Bugzilla API calls, so we know this works). GitHub to Mozilla is a bit more
difficult. But, others have done it: I'm confident we can too.</p>
<p>I see bi-directional syncing of GitHub pull request / code review data
between GitHub and Mozilla as achievable and relatively free from
controversy. I think we should experiment with this sometime in 2015,
probably in Q2, once MozReview is in better condition to <em>host</em> GitHub
pull requests. Although, supporting Git in MozReview is on my Q1 goals
list, so maybe I sneak this into Q1. Time will tell.</p>
<p>At this time, I believe using GitHub for the ingestion of proposed
Firefox commits into existing Mozilla systems should be the limit of
Firefox's GitHub presence, at least as far as day-to-day development
goes. If other groups want to use GitHub more actively and they find a
way to make that work while placating everyone who cares, power to them.
But I think moving the pendulum any further toward GitHub - including
things like making GitHub the exclusive location for code review data,
utilizing GitHub Issues, and making Git[Hub] the canonical Firefox
repository - remain difficult and controverial propositions. I believe
each of these to be medium to high cost and risk with low to medium
reward. I believe it would be wise to defer these questions until we
have data about the value of GitHub pull requests for Firefox
development.</p>
<p>To summarize, I propose using GitHub pull requests as an alternate,
supported front end to the code contribution pathway. We would eliminate
a lot of <em>process debt</em> for non-Mozillians by supporting a known process.
Mozillians on the review and code submission side of the process
shouldn't have to worry about change because, well, it shouldn't matter
if a commit came from GitHub or elsewhere: it will all appear mostly
the same. I'm not saying that we will never expand our utilization
of GitHub for Firefox development beyond this scope. But I am saying
that I don't think it would be prudent to do so today.</p>
<p>And that's how and why I think Mozilla should utilize GitHub for Firefox
development.</p>
<h2>Addendum</h2>
<p>While I'm here, <strong>it's important to note that GitHub does not and will
likely never solve many items from our list of Firefox contribution
process debt</strong>. GitHub is not a build system nor a tool for running
and analyzing code and tests. We still have many, many deficiencies
and usability concerns here. We have historically under-invested in
this area and utilizing GitHub in any capacity won't address these
other issues. In addition, <strong>Firefox is a magnitude larger and more
complex than the vast majority of projects on GitHub. We will always
be burdened with the cost of our success - of coping with and
maintaining the additional complexity associated with that scale</strong>.
Firefox is <em>at least</em> the 0.1%. There's a good chance GitHub and/or many
of the amazing services associated with it (like Travis-CI) won't scale to
our needs. I'd love to be proved wrong here, but the reality is
supporting a marginal use case like Firefox likely isn't at the top of
goals for GitHub and related organizations unless it is in their
business interest (read: financial interest) to do so. One can hope that
as these companies try to capture more of the enterprise market via
offerings such as GitHub Enterprise that they invest in the features
and scalability that large projects and organizations like Mozilla and
Firefox need.</p>]]></content:encoded>
    </item>
    <item>
      <title>Code First and the Rise of the DVCS and GitHub</title>
      <link>http://gregoryszorc.com/blog/2015/01/10/code-first-and-the-rise-of-the-dvcs-and-github</link>
      <pubDate>Sat, 10 Jan 2015 12:35:00 PST</pubDate>
      <category><![CDATA[Git]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/10/code-first-and-the-rise-of-the-dvcs-and-github</guid>
      <description>Code First and the Rise of the DVCS and GitHub</description>
      <content:encoded><![CDATA[<p>The ascendancy of GitHub has very little to do with its namesake tool,
Git.</p>
<p>What GitHub did that was so radical for its time and the strategy that
GitHub continues to execute so well on today is the approach of
putting <em>code first</em> and enabling change to be a frictionless process.</p>
<p>In case you weren't around for the pre-GitHub days or don't remember,
they were not pleasant. Tools around code management were a far cry
from where they are today (I still argue the tools are pretty bad, but
that's for another post). Centralized version control systems were
prevalent (CVS and Subversion in open source, Perforce, ClearCase,
Team Foundation Server, and others in the corporate world). Tools for
looking at and querying code had horrible, ugly interfaces and came out
of a previous era of web design and browser capabilities. It felt like
a chore to do anything, including committing code. Yes, the world
had awesome services like <a href="https://sourceforge.net/">SourceForge</a>,
but they weren't the same as GitHub is today.</p>
<p>Before I get to my central thesis, I want to highlight some supporting
reasons for GitHub's success. There were two developments in the second
half of the 2000s the contributed to the success of GitHub: the rises
of the distributed version control system (DVCS) and the modern web.</p>
<p>While distributed version control systems like Sun WorkShop TeamWare and
BitKeeper existed earlier, it wasn't until the second half of the 2000s
that DVCS systems took off. You can argue part of the reason for this
was open source: my recollection is there wasn't a well-known DVCS
available as free software before 2005. Speaking of 2005, it was a big
year for DVCS projects: Git, Mercurial, and Bazaar all had initial
releases. Suddenly, there were old-but-new ideas on how to do source
control being exposed to new and willing-to-experiment audiences. DVCS
were a critical leap from traditional version control because they
(theoretically) impose less process and workflow limitations on users.
With traditional version control, you needed to be online to commit,
meaning you were managing patches, not commits, in your local
development workflow. There were some forms of branching and merging,
but they were a far cry from what is available today and were often too
complex for mere mortals to use. As more and more people were exposed to
<em>distributed</em> version control, they welcomed its less-restrictive and
more powerful workflows. They realized that source control tools don't
have to be so limiting. <em>Distributed</em> version control also promised all
kinds of revamped workflows that could be harnessed. There were
potential wins all around.</p>
<p>Around the same time that open source DVCS systems were emerging, web
browsers were evolving from an application to render static pages to a
platform for running web <em>applications</em>. Web sites using JavaScript
to dynamically manipulate web page content (DHTML as it was known back
then) were starting to hit their stride. I believe it was GMail that
turned the most heads as to the full power of the <em>modern web</em>
experience, with its novel-for-its-time extreme reliance on
XMLHttpRequest for dynamically changing page content. People were
realizing that powerful, desktop-like applications could be built for
the web and could run everywhere.</p>
<p>GitHub launched in April 2008 standing on the shoulders of both the
emerging interest in the Git content tracking tool and the capabilities
of modern browsers.</p>
<p>I wasn't an early user of GitHub. My recollection is that GitHub was
mostly a Rubyist's playground back then. I wasn't a Ruby programmer, so
I had little reason to use GitHub in the early stages. But people did
start using GitHub. And in the spirit of Ruby (on Rails), GitHub moved
fast, or at least was projecting the notion that they were. While other
services built on top of DVCS tools - like Bitbucket - did exist back then,
GitHub seemed to have momentum associated with it. (Look at the archives
for <a href="https://github.com/blog">GitHub's</a> and
<a href="https://blog.bitbucket.org/">Bitbucket's</a> respective blogs. GitHub has
hundreds of blog entries; Bitbucket numbers in the dozens.) Developers
everywhere up until this point had all been dealing with sub-optimal tools
and workflows. Some of us realized it. Others hadn't. Many of those who
did saw GitHub as a beacon of hope: we have all these new ideas and new
potentials with distributed version control and here is a service under
active development trying to figure out how to exploit that. Oh, and
it's free for open source. Sign me up!</p>
<p>GitHub did capitalize on a market opportunity. They also capitalized on
the value of marketing and the perception that they were moving fast and
providing features that people - especially in open source - wanted.
This captured the early adopters market. But I think what really set
GitHub apart and led to the success they are enjoying today is their
<em>code first</em> approach and their desire to make contribution easy, and
even fun and sociable.</p>
<p>As developers, our job is to solve problems. We often do that by writing
and changing code. And this often involves working as part of a team, or
collaborating. To collaborate, we need tools. You eventually need some
processes. And as I
<a href="/blog/2015/01/09/firefox-contribution-process-debt/">recently blogged</a>,
this can lead to <em>process debt</em> and inefficiencies associated with them.</p>
<p>Before GitHub, the <em>process debt</em> for contributing to other projects was
high. You often had to subscribe to mailing lists in order to submit
patches as emails. Or, you had to create an account on someone's bug
tracker or code review tool before you could send patches. Then you had
to figure out how to use these tools and any organization or
project-specific extensions and workflows attached to them. It was quite
involved and a lot could go wrong. Many projects and organizations (like
Mozilla) still practice this traditional methology. Furthermore (and as
I've
<a href="/blog/2014/10/27/implications-of-using-bugzilla-for-firefox-patch-development/">written before</a>),
these traditional, single patch/commit-based tools often aren't
effective at ensuring the desired output of high quality software.</p>
<p>Before GitHub solved <em>process debt</em> via commoditization of knowledge via
market dominance, they took another approach: emphasizing <em>code first</em>
development.</p>
<p>GitHub is all about the <strong>code</strong>. You load a project page and you see
<strong>code</strong>. You may think a README with basic project information would be
the first thing on a <em>project</em> page. But it isn't. <em>Code</em>, like data,
is king.</p>
<p>Collaboration and contribution on GitHub revolve around the <em>pull
request</em>. It's a way of saying, <em>hey, I made a change, will you take
it?</em> There's nothing too novel in the concept of the <em>pull request</em>:
it's fundamentally no different than sending emails with patches to a
mailing list. But what is so special is GitHub's execution. Gone are
the days of configuring and using one-off tools and processes. Instead,
we have the friendly confines of a clean, friendly, and modern web
experience. While GitHub is built upon the Git tool, you don't even
need to use Git (a tool
<a href="http://git-man-page-generator.lokaltog.net/">lampooned</a>
for its
<a href="http://stevelosh.com/blog/2013/04/git-koans/">horrible usability and approachability</a>)
to contribute on GitHub! Instead, you can
<a href="https://help.github.com/articles/github-flow-in-the-browser/">do everything from your browser</a>.
That warrants repeating: <strong>you don't need to leave your browser to
contribute on GitHub</strong>. GitHub has essentially reduced <em>process debt</em>
to <em>edit a text document</em> territory, and pretty much anybody who has
used a computer can do that. This has enabled GitHub to dabble into
non-code territory, such as its
<a href="https://government.github.com/">GitHub and Government</a> initiative to
foster community involvement in government. (GitHub is really a platform
for easily seeing and changing <em>any</em> content or data. But, please, let
me continue using <em>code</em> as a stand-in, since I want to focus on the
developer audience.)</p>
<p>GitHub took an overly-complicated and fragmented world of varying
contribution processes and made the new world revolve around code and a
unified and simple process for change - the <em>pull request</em>.</p>
<p>Yes, there are other reasons for GitHub's success. You can make strong
arguments that GitHub has capitalized on the social and psychological
aspects of coding and human desire for success and happiness. I agree.</p>
<p>You can also argue GitHub succeeded because of Git. That statement is
more or less technically accurate, but I don't think it is a sound
argument. Git may have been the most feature complete open source
DVCS at the time GitHub came into existence. But that doesn't mean there
is something special about Git that no other DVCS has that makes GitHub
popular. Had another tool been more feature complete or had the backing
of a project as large as Linux at the time of GitHub's launch, we could
very well be looking at a successful service built on something that
isn't Git. Git had early market advantage and I argue its popularity
today - a lot of it via GitHub - is largely a result of its early
advantages over competing tools. And, I would go so far to say that when
you consider the poor usability of Git and the pain that its users go
through when first learning it, more accurate statements would be that
<em>GitHub succeeded in spite of Git</em> and <em>Git owes much of its success to
GitHub</em>.</p>
<p>When I look back at the rise of GitHub, I see a service that has
succeeded by putting people first by allowing them to capitalize
on more productive workflows and processes. They've done this by
emphasizing <em>code</em>, not process, as the means for change. Organizations
and projects should take note.</p>]]></content:encoded>
    </item>
    <item>
      <title>Firefox Contribution Process Debt</title>
      <link>http://gregoryszorc.com/blog/2015/01/09/firefox-contribution-process-debt</link>
      <pubDate>Fri, 09 Jan 2015 16:45:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/09/firefox-contribution-process-debt</guid>
      <description>Firefox Contribution Process Debt</description>
      <content:encoded><![CDATA[<p>As I was
<a href="/blog/2014/12/30/firefox-source-documentation-versus-mdn/">playing around with source-derived documentation</a>,
I grasped the reality that any attempt to move documentation out of
MDN's wiki and into something derived from source code (despite the
argued technical and quality advantages) would likely be met with fierce
opposition because the change process for Firefox is much more involved
than <em>edit a wiki</em>.</p>
<p>This observation casts light on something important: <strong>the very act of
contributing <em>any</em> change to Firefox is too damn hard.</strong></p>
<p>I've always believed this statement to be true. I even like to think I'm
one of the few people that has consistently tried to do something about
it (inventing mach, overhauling the build system, bootstrap scripting,
MozReview, etc). But I, like many of the seasoned Firefox developers,
often lose sight of this grim reality. (I think it's fair to say that
new contributors often complain about the development experience and
as we grow accustomed to it, the complaint volume and intensity wanes).</p>
<p><strong>But when you have the simplicity of editing a wiki page on MDN juxtaposed
against the Firefox change contribution process of today, the inefficiency
of the Firefox process is clearly seen.</strong></p>
<p>The amount of knowledge required to change Firefox is obscenely absurd. Sure,
some complex components will always be difficult to change. But I'm talking
about <em>any</em> component: the base set of knowledge required to contribute
<em>any</em> change to Firefox is vast. This is <strong>before</strong> we get into any
domain-specific knowledge inside Firefox. <strong>I have always believed and
will continue to believe that this is a grave institutional issue for
Mozilla.</strong> It should go without saying that I believe this is an issue worth
addressing. After all, <strong>any piece of knowledge required for
contribution is essentially an obstacle to completion. Elimination of
required knowledge lowers the barrier to contribution.</strong> This, in turn,
allows increased contribution via more and faster change. This advances
the quality and relevance of Firefox, which enables Mozilla to advance
its <a href="https://www.mozilla.org/mission/">Mission</a>.</p>
<p>Seasoned contributors have probably internalized most of the knowledge
required to contribute to Firefox. Here is a <strong>partial</strong> list to remind
everyone of the sheer scope of it:</p>
<p><a name="list-of-process-debt"></a></p>
<ul>
  <li>Before you do anything
  <ul>
   <li>Am I able to contribute?</li>
   <li>Do I meet the minimum requirements (hardware, internet access, etc)?</li>
   <li>Do I need any accounts?</li>
  </ul></li>
  </li>
  <li>Source control
  <ul>
    <li>What is source control?</li>
    <li>How do I install Mercurial/Git?</li>
    <li>How do I use Mercurial/Git?</li>
    <li>Where can I get the Firefox source code?</li>
    <li>How do I *optimally* acquire the Firefox source code?</li>
    <li>Are there any recommended configuration settings or extensions?</li>
  </ul></li>
  <li>Building Firefox
  <ul>
   <li>Do I even need to build Firefox?</li>
   <li>How do I build Firefox?</li>
   <li>What prerequisites need to be installed?</li>
   <li>How do I install prerequisites?</li>
   <li>How do I configure the Firefox build to be appropriate for my needs?</li>
   <li>What are mozconfigs?</li>
   <li>How do I get Firefox to build faster?</li>
   <li>What do I do after a build?</li>
  </ul></li>
  <li>Changing code
  <ul>
   <li>Is there IDE support?</li>
   <li>Where can I find macros and aliases to make things easier?</li>
  </ul></li>
  <li>Testing
  <ul>
    <li>How do I run tests?
  <li>Which tests are relevant for a given change?</li>
  <li>What are all these different test types?</li>
  <li>How do I debug tests?</li>
  </ul></li>
  <li>Try and Automation
  <ul>
   <li>What is Try?</li>
   <li>How do I get an account?</li>
   <li>What is vouching and different levels of access?</li>
   <li>What is SSH?</li>
   <li>How do I configure SSH?</li>
   <li>When will my tests run?</li>
   <li>What is Tree Herder?</li>
   <li>What do all these letters and numbers mean?</li>
   <li>What are all these colors?</li>
   <li>What's an *intermittent failure*?</li>
   <li>How do I know if something is an *intermittent failure*?</li>
   <li>What amount of *intermittent failure* is acceptable?</li>
   <li>What do these logs mean?</li>
   <li>What's buildbot?</li>
   <li>What's mozharness?</li>
  </ul></li>
  <li>Sending patch to Mozilla
  <ul>
   <li>Do I need to sign a CLA?</li>
   <li>Where do I send patches?</li>
   <li>Do I need to get an account on Bugzilla?</li>
   <li>Do I need to file a bug?</li>
   <li>What component should I file a bug in?</li>
   <li>What format should patches be sent in?</li>
   <li>How should I format commit messages?</li>
   <li>How do I upload patches to Bugzilla?</li>
   <li>How does code review work?
   <ul>
    <li>What's the modules system?</li>
    <li>What modules does my change map to?</li>
    <li>Who are the possible reviewers?</li>
    <li>How do I ask someone for review?</li>
    <li>When can I expect review?</li>
    <li>What does r+ vs r- vs f+ vs f- vs cancelling review all mean?</li>
    <li>How do I submit changes to my initial patch?</li>
    <li>What do I do after review?</li>
   </ul></li>
  </ul></li>
  <li>Landing patches
  <ul>
   <li>What repository should a patch land on?</li>
   <li>How do you rebase?</li>
   <li>What's a tree closure?</li>
   <li>What do I do after pushing?</li>
   <li>How do I know the result of the landing?</li>
  </ul></li>
</ul>

<p>Holy #$%@, that's a lot of knowledge. Not only is this list incomplete,
it's also not encompassing a lot of the domain-specific knowledge around
the content being changed.</p>
<p>Every item on this list represents a point where a potential contributor
could throw up their arms out of despair and walk away, giving their
time and talents to another project. Every item on this list that takes
10 minutes instead of 5 could be the tipping point. For common actions,
things that take 5 seconds instead of 1 could be the difference maker.
<strong>This list thus represents reasons that people do not contribute to
Firefox or contribute ineffectively (in the case of common contributors,
like paid Mozilla staff).</strong></p>
<p>I view items on this list as <strong>process debt</strong>. <em>Process debt</em> is a term
I'm coining (perhaps someone has beat me to it - I'm writing this on a
plane without Internet access) that is a sibling of <em>technical debt</em>.
<em>Process debt</em> is overhead and inefficiency associated with <em>processes</em>.
The border between <em>process debt</em> and <em>technical debt</em> in computers is
the code itself (although that border may sometimes not be very
well-defined, as code and process are oftentimes similar, such as most
interactions with version control or code review tools).</p>
<p>When I see this list of <em>process debt</em>, I'm inspired by the opportunity
to streamline the processes and bask in the efficiency gains. But I am
also simultaneously overwhelmed by the vast scope of things that need
improved. When I think about the amount of energy that will need to be
exerted to fight the <em>OMG change</em> crowd, the list becomes <em>depressing</em>.
But discussing institutional resistance to change, the <em>stop energy</em>
associated with it, and Mozilla's historical record of failing to invest
in fixing process (and technical) debt is for another post.</p>
<p>When looking at the above list, I can think of the following general
ways to make it more approachable:</p>
<ol>
<li>Remove an item completely. If it isn't on the list, there is nothing
   to know and no overhead. The best way to solve a problem is to make
   it not exist.</li>
<li>Automate an item and makes its existence largely transparent. If an
   item is invisible, does it exist in the mind of a contributor? (This
   is also known as solving the problem by adding a layer of
   indirection.)</li>
<li>Change an item so that it is identical to another, more familiar
   process. If you use a well-defined process, there is no new knowledge
   that must be learned and the cost of on-boarding someone already
   familiar with that knowledge is practically zero.</li>
</ol>
<p>When you start staring at this list of Firefox contribution process debt,
you start thinking about priorities, groupings, and strategies. You look
around at what others are doing to see if you can borrow good ideas.</p>
<p>I've done a lot of thinking on the subject and have some ideas and
recommendations. Stay tuned for some additional posts on the topic.</p>]]></content:encoded>
    </item>
    <item>
      <title>Style Changes on hg.mozilla.org</title>
      <link>http://gregoryszorc.com/blog/2015/01/09/style-changes-on-hg.mozilla.org</link>
      <pubDate>Fri, 09 Jan 2015 15:25:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2015/01/09/style-changes-on-hg.mozilla.org</guid>
      <description>Style Changes on hg.mozilla.org</description>
      <content:encoded><![CDATA[<p>Starting today and continuing through next week, there will be a number
of styling changes made to <a href="https://hg.mozilla.org/">hg.mozilla.org</a>.</p>
<p>The main goal of the work is to bring the style up-to-date with upstream
Mercurial. This will result in more features being available to the web
interface, hopefully making it more useful. This includes display of
bookmarks and the Mercurial help documentation. As part of this work,
we're also removing some files on the server that shouldn't be used. If
you start getting 404s or notice an unexpected theme change, this is
probably the reason why.</p>
<p>If you'd like to look over the changes before they are made or would
like to file a bug against a regression (we suspect there will be
minor regressions due to the large nature of the changes), head on
over to <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1117021">bug 1117021</a>
or ping people in #vcs on IRC.</p>]]></content:encoded>
    </item>
    <item>
      <title>Mercurial Pushlog Is Now Robust Against Interrupts</title>
      <link>http://gregoryszorc.com/blog/2014/12/30/mercurial-pushlog-is-now-robust-against-interrupts</link>
      <pubDate>Tue, 30 Dec 2014 12:25:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <category><![CDATA[Firefox]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/12/30/mercurial-pushlog-is-now-robust-against-interrupts</guid>
      <description>Mercurial Pushlog Is Now Robust Against Interrupts</description>
      <content:encoded><![CDATA[<p><a href="https://hg.mozilla.org">hg.mozilla.org</a> - Mozilla's Mercurial server -
has functionality called the <a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmo/pushlog.html">pushlog</a>
which records who pushed what when. Essentially, it's a log of when
a repository was changed. This is separate from the commit log because
the commit log can be spoofed and the commit log doesn't record when
commits were actually pushed.</p>
<p>Since its inception, the pushlog has suffered from data consistency
issues. If you aborted the push at a certain time, data was not inserted
in the pushlog. If you aborted the push at another time, data existed in
the pushlog but not in the repository (the repository would get rolled
back but the pushlog data wouldn't).</p>
<p><strong>I'm pleased to announce that the pushlog is now robust against
interruptions and its updates are consistent with what is recorded by
Mercurial.</strong> The pushlog database commit/rollback is tied to Mercurial's
own transaction API. What Mercurial does to the push transaction, the
pushlog follows.</p>
<p>This former inconsistency has caused numerous problems over the years.
When data was inconsistent, we often had to close trees until someone
could SSH into the machines and manually run SQL to fix the problems.
This also contributed to a culture of <em>don't press ctrl+c during push:
it could corrupt Mercurial.</em> (Ctrl+c should be safe to press any time: if
it isn't, there is a bug to be filed.)</p>
<p>Any time you remove a source of tree closures is a cause for celebration.
Please join me in celebrating your new freedom to abort pushes without
concern for data inconsistency.</p>
<p>In case you want to test things out, aborting pushes and (and rolling
back the pushlog) should now result in something like:</p>
<pre><code>pushing to ssh://hg.mozilla.org/mozilla-central
searching for changes
adding changesets
adding manifests
adding file changes
added 1 changesets with 1 changes to 1 files
Trying to insert into pushlog.
Inserted into the pushlog db successfully.
^C
rolling back pushlog
transaction abort!
rollback completed
</code></pre>]]></content:encoded>
    </item>
    <item>
      <title>Firefox Source Documentation Versus MDN</title>
      <link>http://gregoryszorc.com/blog/2014/12/30/firefox-source-documentation-versus-mdn</link>
      <pubDate>Tue, 30 Dec 2014 12:00:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/12/30/firefox-source-documentation-versus-mdn</guid>
      <description>Firefox Source Documentation Versus MDN</description>
      <content:encoded><![CDATA[<p>The Firefox source tree has had in-tree documentation powered by
<a href="https://gecko.readthedocs.org/en/latest/">Sphinx</a> for a while now.
However, its canonical home has been a
<a href="https://ci.mozilla.org/job/mozilla-central-docs/Tree_Documentation/index.html">hard-to-find URL on ci.mozilla.org</a>.
I finally <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1115278">scratched an itch</a>
and wrote patches to enable the docs to be built easier. So,
starting today, the docs are now available on Read the Docs at
<a href="https://gecko.readthedocs.org/en/latest/">https://gecko.readthedocs.org/en/latest/</a>!</p>
<p>While I was scratching itches, I decided to play around with another
documentation-related task: automatic API documentation. I have a
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1115452">limited proof-of-concept for automatically generating XPIDL interface
documentation</a>.
Essentially, we use the in-tree XPIDL parser to parse <em>.idl</em> files and
turn the Python object representation into reStructured Text, which Sphinx
parses and renders into pretty HTML for us. The concept can be applied to any
source input, such as WebIDL and JavaScript code. I chose XPIDL because a
parser is readily available and I know Joshua Cranmer has expressed
interest in automatic XPIDL documentation generation. (As an aside,
JavaScript tooling that supports the flavor of JavaScript used
internally by Firefox is very limited. We need to prioritize removing
Mozilla extensions to JavaScript if we ever want to start using awesome
tooling that exists in the wild.)</p>
<p>As I was implementing this proof-of-concept, I was looking at XPIDL
interface documentation on MDN to see how things are presented today.
After perusing MDN for a bit and comparing its content against what I
was able to derive from the source, something became extremely clear:
MDN has significantly more content than the canonical source code.
Obviously the <em>.idl</em> files document the interfaces, their attributes,
their methods, and all the types and names in between: that's the very
definition of an IDL. But what was generally missing from the source
code is comments. <em>What does this method do?</em> <em>What is each argument
used for?</em> Things like example usage are almost non-existent in the
source code. MDN, by contrast, typically has no shortage of all these
things.</p>
<p>As I was grasping the reality that MDN has a lot of out-of-tree
supplemental content, I started asking myself <em>what's the point
in automatic API docs? Is manual document curation on MDN good
enough?</em> <strong>This question has sort of been tearing me apart.</strong> Let me try
to explain.</p>
<p>MDN is an amazing site. You can tell a lot of love has gone into making
the experience and much of its content excellent. However, the content
around the technical implementation / internals of Gecko/Firefox generally
sucks. There are some exceptions to the rule. But I find that things like
internal API documentation to be lackluster on average. It is rare for
me to find documentation that is up-to-date and useful. It is common to
find documentation that is partial and incomplete. It is very common to
find things like JSMs not documented at all. <strong>I think this is a
problem.</strong> I argue the lack of good documentation raises the barrier to
contributing. <strong>Furthermore, writing and maintaining excellent
low-level documentation is too much effort.</strong></p>
<p><strong>My current thoughts on API and low-level documentation are that I
question the value of this documentation existing on MDN.</strong>
Specifically, I think things like JSM API docs
(like <a href="https://developer.mozilla.org/en-US/docs/Mozilla/JavaScript_code_modules/Sqlite.jsm">Sqlite.jsm</a>)
and XPIDL interface documentation (like
<a href="https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XPCOM/Reference/Interface/nsIFile">nsIFile</a>)
don't belong on MDN - at least not in wiki form. <strong>Instead, I believe
that documentation like this should live in and be derived from source
code.</strong> Now, if the MDN site wants to expose this as read-only content
or if MDN wants to enable the content to be annotated in a wiki-like
manner (like how MSDN and PHP documentation allow user comments), that's
perfectly fine by me. Here's why.</p>
<p>First, if I must write separate-from-source-code API documentation on
MDN (or any other platform for that matter), I must now perform extra
work or forgo either the source code or external documentation. In other
words, if I write in-line documentation in the source code, I must spend
extra effort to essentially copy large parts of that to MDN. And I must
continue to spend extra effort to keep updates in sync. If I don't
want to spend that extra effort (I'm as lazy as you), I have to choose
between documenting the source code or documenting MDN. If I choose the
source code, people either have to read the source to read the docs
(because we don't generate documentation from source today) or someone
else has to duplicate the docs (overall more work). If I choose to
document on MDN, then people reading the source code (probably because
they want to change it) are deprived of additional context useful to
make that process easier. <strong>This is a lose-lose scenario and it is a
general waste of expensive people time.</strong></p>
<p>Second, I prefer having API documentation derived from source code
because I feel it results in more accurate documentation that has the
higher liklihood of remaining accurate and in sync with reality. Think
about it: when was the last time you reviewed changes to a JSM and
searched MDN for content that needed updated? I'm sure there are some
pockets of people that do this right. But I've written dozens of
JavaScript patches for Firefox and I'm pretty sure I've been asked to
update <em>external</em> documentation less than 5% of the time. Inline source
documentation, however, is another matter entirely. Because the
documentation is often proximal to code that changed, I frequently a) go
ahead and make the documentation changes because everything is right there
and it's low overhead to change as I adjust the source b) am asked to
update in-line docs when a reviewer sees I forgot to. Generally
speaking, things tend to stay in sync and fewer bugs form when
everything is proximally located. By fragmenting documentation between
source code and external services like MDN, we increase the liklihood
that things become out of sync. This results in misleading information
and increases the barriers to contribution and change. In other words,
developer inefficiency.</p>
<p>Third, having API documentation derived from source code opens up
numerous possibilities to further aid developer productivity and improve
the usefullness of documentation. For example:</p>
<ul>
<li>We can parse <em>@param</em> references out of documentation and issue
  warnings/errors when documentation doesn't match the AST.</li>
<li>We can issue warnings when code isn't documented.</li>
<li>We can include in-line examples and execute and verify these as part
  of builds/tests.</li>
<li>We can more easily cross-reference APIs because everything is
  documented consistently. We can also issue warnings when
  cross-references no longer work.</li>
<li>We can derive files so editors and IDEs can display in-line API docs
  as you type or can complete functions as you type, allowing people to
  code faster.</li>
</ul>
<p>While we don't generally do these things today, they are all within the
realm of possibility. Sphinx supports doing many of these things. Stop
reading and run <em>mach build-docs</em> right now and look at the warnings
from malformed documentation. I don't know about you, but I love when my
tools tell me when I'm creating a burden for others.</p>
<p>There really is so much more we could be doing with source-derived
documentation. And I argue managing it would take less overall work and
would result in higher quality documentation.</p>
<p>But the world of source-derived documentation isn't all roses. MDN has a
very important advantage: it's a wiki. Just log in, edit in a WYSIWYG,
and save. It's so easy. The moment we move to source-derived
documentation, we introduce the massive Firefox source repository, the
Firefox code review process, bugs/Bugzilla, version control overhead
(although versioning documentation is another plus for source-derived
documentation), landing changes, extra cost to Mozilla for building and
running those checkins (even if they contain docs-only changes, sadly),
and the time and cognitive burden associated with each one. That's a
lot of extra work compared to clicking a few buttons on MDN! <strong>Moving
documentation editing out of MDN and into the Firefox patch submission
world would be a step in the wrong direction in terms of fostering
contributions.</strong> Should someone really have to go through all that just
to correct a typo? I have no doubt we'd lose contributors if we switched
the change contribution process. And considering our lackluster track
record of writing inline documentation in source, I don't feel great
about losing <em>any</em> person who contributes documentation, no matter how
small the contribution.</p>
<p>And this is my dilemma: the existing source-or-MDN solution is sub-par
for pretty much everything except ease of contribution on MDN and
deploying nice tools (like Sphinx) to address the suckitude will result
in more difficulty contributing. Both alternatives suck.</p>
<p>I intend to continue this train of thought in a subsequent post. Stay
tuned.</p>]]></content:encoded>
    </item>
    <item>
      <title>Why hg.mozilla.org is Slow</title>
      <link>http://gregoryszorc.com/blog/2014/12/19/why-hg.mozilla.org-is-slow</link>
      <pubDate>Fri, 19 Dec 2014 14:40:00 PST</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/12/19/why-hg.mozilla.org-is-slow</guid>
      <description>Why hg.mozilla.org is Slow</description>
      <content:encoded><![CDATA[<p>At Mozilla, I often hear statements like <em>Mercurial is slow.</em> That's a
very general statement. Depending on the context, it can mean one or
more of several things:</p>
<ul>
<li>My Mercurial workflow is not very efficient</li>
<li><em>hg</em> commands I execute are slow to run</li>
<li><em>hg</em> commands I execute appear to stall</li>
<li>The Mercurial server I'm interfacing with is slow</li>
</ul>
<p>I want to spend time talking about a specific problem: why
hg.mozilla.org (the server) is slow.</p>
<h2>What Isn't Slow</h2>
<p><strong>If you are talking to hg.mozilla.org over HTTP or HTTPS
(https://hg.mozilla.org/), there should not currently be any server
performance issues</strong>. Our Mercurial HTTP servers are pretty beefy and
are able to absorb a lot of load.</p>
<p>If https://hg.mozilla.org/ is slow, chances are:</p>
<ul>
<li>You are doing something like cloning a 1+ GB repository.</li>
<li>You are asking the server to do something really expensive (like
  generate JSON for 100,000 changesets via the pushlog query interface).</li>
<li>You don't have a high bandwidth connection to the server.</li>
<li>There is a network event.</li>
</ul>
<h2>Previous Network Events</h2>
<p>There have historically been network capacity issues in the datacenter
where hg.mozilla.org is hosted (SCL3).</p>
<p>During Mozlandia, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1107156">excessive traffic to ftp.mozilla.org</a>
essentially saturated the SCL3 network. During this time, requests to
hg.mozilla.org were timing out: Mercurial traffic just couldn't traverse
the network. Fortunately, events like this are quite rare.</p>
<p>Up until recently, Firefox release automation was effectively
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1096337">overwhelming the network</a>
by doing some clownshoesy things.</p>
<p>For example, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1096653">gaia-central was being cloned all the time</a>
We had a ~1.6 GB repository being cloned over a thousand times per day. We
were transferring close to 2 TB of gaia-central data out of Mercurial servers
per day</p>
<p>We also found issues with
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1096650">pushlogs sending 100+ MB responses</a>.</p>
<p>And the <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1100574">build/tools repo was getting cloned for every job</a>. Ditto for mozharness.</p>
<p>In all, <strong>we identified a few terabytes of excessive Mercurial traffic
that didn't need to exist</strong>. This excessive traffic was saturating the
SCL3 network and slowing down not only Mercurial traffic, but other
traffic in SCL3 as well.</p>
<p>Fortunately, people from Release Engineering were quick to respond to
and fix the problems once they were identified. The problem is now
firmly in control. Although, given the scale of Firefox's release
automation, any new system that comes online that talks to version
control is susceptible to causing server outages. I've already raised
this concern when reviewing some TaskCluster code. The thundering herd
of automation will be an ongoing concern. But I have plans to further
mitigate risk in 2015. Stay tuned.</p>
<p>Looking back at our historical data, it appears that we hit these
network saturation limits a few times before we reached a tipping point
in early November 2014. Unfortunately, we didn't realize this because up
until recently, we didn't have a good source of data coming from the
servers. We lacked the tooling to analyze what we had. We lacked the
experience to know what to look for. Outages are effective flashlights.
We learned a lot and know what we need to do with the data moving
forward.</p>
<h2>Available Network Bandwidth</h2>
<p>One person pinged me on IRC with the comment <em>Git is cloning much faster
than Mercurial.</em> I asked for timings and the Mercurial clone wall time
for Firefox was much higher than I expected.</p>
<p>The reason was network bandwidth. This person was performing a Git clone
between 2 hosts in EC2 but was performing the Mercurial clone between
hg.mozilla.org and a host in EC2. In other words, they were partially
comparing the performance of a 1 Gbps network against a link over the
public internet! When they did a fair comparison by removing the network
connection as a variable, the clone times rebounded to what I expected.</p>
<p>The single-homed nature of hg.mozilla.org in a single datacenter in
northern California is not only bad for disaster recovery reasons, it
also means that machines far away from SCL3 or connecting to SCL3 over a
slow network aren't getting optimal performance.</p>
<p>In 2015, expect us to build out a geo-distributed hg.mozilla.org so that
connections are hitting a server that is closer and thus faster. This
will probably be targeted at Firefox release automation in AWS first. We
want those machines to have a fast connection to the server <strong>and</strong> we
want their traffic isolated from the servers developers use so that
hiccups in automation don't impact the ability for humans to access
and interface with source code.</p>
<h2>NFS on SSH Master Server</h2>
<p>If you connect to http://hg.mozilla.org/ or https://hg.mozilla.org/, you
are hitting a pool of servers behind a load balancer. These servers have
repository data stored on local disk, where I/O is fast. In reality,
most I/O is serviced by the page cache, so local disks don't come into
play.</p>
<p>If you connect to ssh://hg.mozilla.org/, you are hitting a single,
master server. Its repository data is hosted on an NFS mount. I/O on the
NFS mount is horribly slow. Any I/O intensive operation performed on the
master is much, much slower than it should be. Such is the nature of
NFS.</p>
<p>We'll be exploring ways to mitigate this performance issue in 2015. But
it isn't the biggest source of performance pain, so don't expect anything
immediately.</p>
<h2>Synchronous Replication During Pushes</h2>
<p>When you <em>hg push</em> to hg.mozilla.org, the changes are first made on the
SSH/NFS master server. They are subsequently mirrored out to the HTTP
read-only slaves.</p>
<p>As is currently implemented, the mirroring process is performed
synchronously during the push operation. The server waits for the
mirrors to complete (to a reasonable state) before it tells the client
the push has completed.</p>
<p>Depending on the repository, the size of the push, and server and
network load, <strong>mirroring commonly adds 1 to 7 seconds to push times</strong>.
This is time when a developer is sitting at a terminal, waiting for <em>hg
push</em> to complete. The time for Try pushes can be larger: 10 to 20
seconds is not uncommon (but fortunately not the norm).</p>
<p>The current mirroring mechanism is overly simple and prone to many
failures and sub-optimal behavior. I plan to work on fixing mirroring in
2015. When I'm done, there should be no user-visible mirroring delay.</p>
<h2>Pushlog Replication Inefficiency</h2>
<p>Up until yesterday (when we
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1112415">deployed a rewritten pushlog extension</a>,
the replication of pushlog data from master to server was very
inefficient. Instead of tranferring a delta of pushes since last pull,
we were literally copying the underlying SQLite file across the network!</p>
<p>Try's pushlog is ~30 MB. mozilla-central and mozilla-inbound are in the
same ballpark. 30 MB x 10 slaves is a lot of data to transfer. These
operations were capable of periodically saturating the network, slowing
everyone down.</p>
<p>The rewritten pushlog extension performs a delta transfer automatically
as part of <em>hg pull</em>. Pushlog synchronization now completes in
milliseconds while commonly only consuming a few kilobytes of network
traffic.</p>
<p>Early indications reveal that deploying this change yesterday decreased the
push times to repositories with long push history by 1-3s.</p>
<h2>Try</h2>
<p>Pretty much any interaction with the
<a href="https://hg.mozilla.org/try">Try repository</a> is guaranteed to have poor
performance. The Try repository is doing things that distributed
versions control systems weren't designed to do. This includes Git.</p>
<p>If you are using Try, all bets are off. Performance will be problematic
until we roll out the <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1055298">headless try</a>
repository.</p>
<p>That being said, we've made changes recently to make Try perform better.
<a href="https://twitter.com/lxt/status/546042588822642688">The median time for pushing to Try</a>
has decreased significantly in the past few weeks. The first dip in
mid-November was due to upgrading the server from Mercurial 2.5 to
Mercurial 3.1 and from converting Try to use <em>generaldelta</em> encoding.
The dip this week has been from merging all heads and from deploying the
aforementioned pushlog changes. <strong>Pushing to Try is now significantly
faster than 3 months ago.</strong></p>
<h2>Conclusion</h2>
<p>Many of the reasons for hg.mozilla.org slowness are known. More often
than not, they are due to clownshoes or inefficiencies on Mozilla's
part rather than fundamental issues with Mercurial.</p>
<p><strong>We have made significant progress at making hg.mozilla.org faster.</strong>
But we are not done. We are continuing to invest in fixing the
sub-optimal parts and making hg.mozilla.org faster yet. I'm confident
that within a few months, nobody will be able to say that the servers
are a source of pain like they have been for years.</p>
<p>Furthermore, Mercurial is investing in features to make the wire
protocol faster, more efficient, and more powerful. When deployed,
these should make pushes faster on <em>any</em> server. They will also enable
workflow enhancements, such as Facebook's experimental extension to
perform rebases as part of push (eliminating push races and having to
manually rebase when you lose the push race).</p>]]></content:encoded>
    </item>
    <item>
      <title>mach sub-commands</title>
      <link>http://gregoryszorc.com/blog/2014/12/18/mach-sub-commands</link>
      <pubDate>Thu, 18 Dec 2014 09:45:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/12/18/mach-sub-commands</guid>
      <description>mach sub-commands</description>
      <content:encoded><![CDATA[<p><a href="https://pypi.python.org/pypi/mach/">mach</a> - the generic command line
dispatching tool that powers the <strong>mach</strong> command to aid Firefox
development - now has support for sub-commands.</p>
<p>You can now create simple and intuitive user interfaces involving
sub-actions. e.g.</p>
<pre><code>mach device sync
mach device run
mach device delete
</code></pre>
<p>Before, to do something like this would require a universal argument
parser or separate mach commands. Both constitute a poor user
experience (confusing array of available arguments or proliferation of
top-level commands). Both result in <em>mach help</em> being difficult to
comprehend. And that's not good for usability and approachability.</p>
<p>Nothing in Firefox currently uses this feature. Although there is an
in-progress patch in
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1108293">bug 1108293</a>
for providing a mach command to analyze C/C++ build dependencies.
It is my hope that others write useful commands and functionality
on top of this feature.</p>
<p>The documentation for mach has also been rewritten. It is now
<a href="https://ci.mozilla.org/job/mozilla-central-docs/Tree_Documentation/python/mach/index.html">exposed</a>
as part of the in-tree Sphinx documentation.</p>
<p>Everyone should thank <a href="https://mozillians.org/en-US/u/ahal/">Andrew Halberstadt</a>
for promptly reviewing the changes!</p>]]></content:encoded>
    </item>
    <item>
      <title>A Crazy Day</title>
      <link>http://gregoryszorc.com/blog/2014/12/04/a-crazy-day</link>
      <pubDate>Thu, 04 Dec 2014 23:34:00 PST</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/12/04/a-crazy-day</guid>
      <description>A Crazy Day</description>
      <content:encoded><![CDATA[<p>Today was one crazy day.</p>
<p>The build peers all sat down with Release Engineering and Axel Hecht
to talk l10n packaging. <a href="http://glandium.org/">Mike Hommey</a> has a
Q1 goal to fix l10n packaging. There is buy-in from Release
Engineering on enabling him in his quest to slay dragons. This
will make builds faster and will pay off a massive mountain of
technical debt that plagues multiple groups at Mozilla.</p>
<p>The Firefox build system contributors sat down with a bunch of
Rust people and we talked about what it would take to integrate
Rust into the Firefox build system and the path towards shipping
a Rust component in Firefox. It sounds like that is going to
happen in 2015 (although we're not yet sure what component will
be written in Rust). I consider it an achievement that the gathering
of both groups didn't result in infinite rabbit holing about
system architectures, toolchains, and the build people telling
<em>horror stories</em> to wide-eyed Rust people about the crazy things
we have to do to build and ship Firefox. Believe me, the
temptation was there.</p>
<p>People interested in the build system all sat down and reflected on
the state of the build system and where we want to go. We agreed
to create a build mode optimized for non-Gecko developers
that downloads pre-built binaries - avoiding ~10 minutes of
C/C++ compile time for builds. Mark my words, this will be one
of those changes that once deployed will cause people to say
"I can't believe we went so long without this."</p>
<p>I joined Mark Côté and others to talk about priorities for
MozReview. We'll be making major improvements to the UX and
integrating static analysis into reviews. Push a patch for
review and have machines do some of the work that humans are
currently doing! We're tentatively planning a get-together in
Toronto in January to sprint towards awesomeness.</p>
<p>I ended the day by giving a long and rambling presentation about
version control, with emphasis on Mercurial. I can't help but
feel that I talked way too much. There's just so much knowledge
to cover. A few people told me afterwards they learned a lot. I'd
appreciate feedback if you attended. Anyway, I got a few nods
from people as I was saying some somewhat contentious things.
It's always good to have validation that I'm not on an island
when I say crazy things.</p>
<p>I hope to spend Friday chasing down loose ends from the week.
This includes talking to some security gurus about another crazy
idea of mine to integrate PGP into the code review and code
landing workflow for Firefox. I'll share more details once
I get a professional opinion on the security front.</p>]]></content:encoded>
    </item>
  </channel>
</rss>
