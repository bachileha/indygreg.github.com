<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>Gregory Szorc's Digital Home</title>
    <link>http://gregoryszorc.com/blog</link>
    <description>Rambling on</description>
    <pubDate>Mon, 13 Oct 2014 23:48:56 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Mozilla Mercurial Statistics</title>
      <link>http://gregoryszorc.com/blog/2014/09/30/mozilla-mercurial-statistics</link>
      <pubDate>Tue, 30 Sep 2014 13:17:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/09/30/mozilla-mercurial-statistics</guid>
      <description>Mozilla Mercurial Statistics</description>
      <content:encoded><![CDATA[<p>I recently gained SSH access to Mozilla's Mercurial servers. This allows
me to run some custom queries directly against the data. I was
interested in some high-level numbers and thought I'd share the results.</p>
<p>hg.mozilla.org hosts a total of 3,445 repositories. Of these, there are
1,223 distinct root commits (i.e. distinct graphs). Altogether, there
are 32,123,211 commits. Of those, there are 865,594 distinct commits (not
double counting commits that appear in multiple repositories).</p>
<p>We have a high ratio of total commits to distinct commits (about 37:1).
This means we have high duplication of data on disk. This basically
means a lot of repos are clones/forks of existing ones. No big surprise
there.</p>
<p>What is surprising to me is the low number of total distinct commits. I
was expecting the number to run into the millions. (Firefox itself
accounts for ~240,000 commits.) Perhaps a lot of the data is sitting in
Git, Bitbucket, and GitHub. Sounds like a good data mining expedition...</p>]]></content:encoded>
    </item>
    <item>
      <title>On Monolithic Repositories</title>
      <link>http://gregoryszorc.com/blog/2014/09/09/on-monolithic-repositories</link>
      <pubDate>Tue, 09 Sep 2014 10:00:00 PDT</pubDate>
      <category><![CDATA[Git]]></category>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/09/09/on-monolithic-repositories</guid>
      <description>On Monolithic Repositories</description>
      <content:encoded><![CDATA[<p>When companies or organizations deploy version control, they have to
make many choices. One of them is how many repositories to create.
Your choices are essentially a) a single, monolithic repository that
holds everything b) many separate, smaller repositories that hold
all the individual parts c) something in between.</p>
<p>The prevailing convention today (especially in the open source
realm) is to create many separate and loosely coupled repositories,
each repository mapping to a specific product or service. That does
seem reasonable: if you were organizing files on your filesystem,
you would group them by functionality or role (photos, music,
documents, etc). And, version control tools are functionally
filesystems. So it makes sense to draw repository boundaries at
directory/role levels.</p>
<p>Further reinforcing the separate repository convention is the
scaling behavior of our version control tools. Git, the popular
tool in open source these days, doesn't scale well to very large
repositories due to - among other things - not having narrow clones
(fetching a subset of files). It scales well enough to the
overwhelming majority of projects. But if you are a large
organization generating lots of data (read: gigabytes of data over
hundreds of thousands of files and commits) for version control,
Git is unsuitable in its current form. Other tools (like Mercurial)
don't currently fare that much better (although Mercurial has plans
to tackle these scaling vectors).</p>
<p>Despite popular convention and even limitations in tools, companies
like Google and Facebook opt to run large, monolithic repositories.
Google runs Perforce.
<a href="https://code.facebook.com/posts/218678814984400/scaling-mercurial-at-facebook/">Facebook is on Mercurial</a>,
or at least is in the process of migrating to Mercurial.</p>
<p>Why do these companies run monolithic repositories?
In <a href="http://www.perforce.com/sites/default/files/still-all-one-server-perforce-scale-google-wp.pdf">Google's words</a>:</p>
<p><em>We have a single large depot with almost all of Google's projects
on it. This aids agile development and is much loved by our users,
since it allows almost anyone to easily view almost any code, allows
projects to share code, and allows engineers to move freely from
project to project. Documentation and data is stored on the server
as well as code.</em></p>
<p>So, monolithic repositories are all about moving fast and getting things
done more efficiently. In other words, <strong>monolithic repositories
increase developer productivity.</strong></p>
<p>Furthermore, monolithic repositories are also more compatible with
the ebb and flow of large organizations and large software projects.
Components, features, products, and teams come and go, merge and split.
The only constant is change. And if you are maintaining separate
repositories that attempt to map to this ever-changing organizational
topology, you are going to have a bad time. Either you'll be
constantly copying, moving, merging, splitting, etc data and repositories.
Or your repositories will be organized in a very non-logical and
non-intuitive manner. That translates to overhead and lost productivity.
I think that monolithic repositories handle the realities of large
organizations much better. Big change or reorganization you want
to reflect? You can make a single, atomic, history-preserving commit
to move things around. I think that's much more manageable, especially
when you consider the difficulty and annoyance of history-preserving
changes across repositories.</p>
<p>Naysayers will decry monolithic repositories on principled and practical
grounds.</p>
<p>The principled camp will say that separate repositories
constitute a loosely coupled (dare I say service oriented) architecture
that maps better to how software is consumed, assembled, and deployed
and that erecting barriers in the form of separate repositories
deliberately enforces this architecture. I agree. However, you can
still maintain a loosely coupled architecture with monolithic
repositories. The Subversion model of checking out a single tree
<em>from a larger repository</em> proves this. Furthermore, I would say
architecture decisions should be enforced by people (via code review,
etc), not via version control repository topology. I believe this
principled argument against monolithic repositories to be rather weak.</p>
<p>The principled camp living in the open source realm may also decry
monolithic repositories as an affront to the spirit of open source.
They would say that a monolithic repository creates unfairly strong
ties to the organization that operates it and creates barriers to
forking, etc. This may be true. But monolithic repositories don't
intrinsically infringe on the
<a href="https://www.gnu.org/philosophy/free-sw.html">basic software freedoms</a>,
organizations do. Therefore, I find this principled argument rather
weak.</p>
<p>The practical camp will say that monolithic repositories just don't
scale or aren't suitable for general audiences. These concerns are
real.</p>
<p><em>Fully</em> distributed version control systems (every commit on every
machine) definitely don't scale past certain limits. Depending on your
repository and user base, your scaling limits include disk space
(repository data terabytes in size), bandwidth (repository data terabytes
in size), filesystem (repository hundreds of thousands or millions of
files), CPU and memory (operations on large repositories take too
many system resources), and many heads/branches (tools like Git and
Mercurial don't scale well to tens of thousands of heads/branches).
These limitations with fully distributed version
control are why distributed version control tools like Git and
Mercurial support a partially-distributed mode that behaves more like
your classical server-client model, like those employed by Subversion,
Perforce, etc. Git supports shallow clone and sparse checkout.
Mercurial supports shallow clone (via remotefilelog) and has planned
support for narrow clone and sparse checkout in the next release or
two. Of course, you can avoid the scaling limitations of distributed
version control by employing a non-distributed tool, such as Subversion.
Many companies continue to reach this conclusion today. However,
users adapted to the distributed workflow would likely be
up in arms (they would probably use tools like hg-subversion or git-svn
to maintain their workflows). So, while scaling of version control
can be a real concern, there are solutions and workarounds. However,
they do involve falling back to a partially-distributed model.</p>
<p>Another concern with monolithic repositories is user access control. You
inevitably have code or data that is more sensitive and want to limit
who can change or even access it. Separate repositories seem to
facilitate a simpler model: per-repository access control. With
monolithic repositories, you have to worry about per-directory/subtree
permissions, an increased risk of data leaking, etc. This concern is
more real with distributed version control, as distributed data and
access control aren't naturally compatible. But these issues can be
resolved. And if the tooling supports it, there is only a semantic
difference between managing access control between repositories versus
components of a single repository.</p>
<p>When it comes to repository hosting conversions, I agree with Google
and Facebook: <strong>I prefer monolithic repositories</strong>. When I am interacting
with version control, I just want to get stuff done. I don't want to
waste time dealing with multiple commands to manage multiple
repositories. I don't want to waste time or expend cognitive load
dealing with submodule, subrepository, or big files management. I
don't want to waste time trying to find and reuse code, data, or
documentation. I want everything at my fingertips, where it can be
easily discovered, inspected, and used. Monolithic repositories
facilitate these workflows more than separate repositories and make
me more productive as a result.</p>
<p>Now, if only all the tools and processes we use and love would work
with monolithic repositories...</p>]]></content:encoded>
    </item>
    <item>
      <title>Reproducing Mozilla's Mercurial Server</title>
      <link>http://gregoryszorc.com/blog/2014/09/05/reproducing-mozilla's-mercurial-server</link>
      <pubDate>Fri, 05 Sep 2014 14:50:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/09/05/reproducing-mozilla's-mercurial-server</guid>
      <description>Reproducing Mozilla's Mercurial Server</description>
      <content:encoded><![CDATA[<p>Of of my first tasks in my
<a href="/blog/2014/09/05/new-job-role/">new role</a> as a <em>Developer Productivity
Engineer</em> is to help make Mozilla's Mercurial server better. Many of the
awesome things we have planned rely on features in newer versions of
Mercurial. It's therefore important for us to upgrade our Mercurial
server to a modern version (we are currently running 2.5.4) and to keep
our Mercurial server upgraded as time passes.</p>
<p>There are a few reasons why we haven't historically upgraded our
Mercurial server. First, as anyone who has maintained high-availability
systems will tell you, there is the attitude of <em>if it isn't broken,
don't fix it.</em> In other words, Mercurial 2.5.4 is working fine, so why
mess with a good thing. This was all fine and dandy - until Mercurial
started falling over in the last few weeks.</p>
<p>But the blocker towards upgrading that I want to talk about today is
systems verification. There has been extreme caution around upgrading
Mercurial at Mozilla because it is a critical piece of Mozilla's
infrastructure and if the upgrade were to not go well, the outage
would be disastrous for developer productivity and could even jeopardize
an emergency Firefox release.</p>
<p>As much as I'd like to say that a modern version of Mercurial on the
server would be a drop-in replacement (Mercurial has a great committment
to backwards compatibility and has loose coupling between clients and
servers such that upgrading servers should not impact clients), there is
always a risk that something will change. And that risk is compounded by
the amount of custom code we have running on our server.</p>
<p>The way you protect against unexpected changes is testing. In the ideal
world, you have a robust test suite that you run against a staging
instance of a service to validate that any changes have no impact. In
the absence of testing, you are left with fear, uncertainty, and doubt.
FUD is an especially horrible philosophy when it comes to managing
servers.</p>
<p>Unfortunately, we don't really have a great testing infrastructure for
Mozilla's Mercurial server. And I want to change that.</p>
<h2>Reproducing the Server Environment</h2>
<p>When writing tests, it is important for the thing being tested to be as
similar as possible to the real thing. This is why so many people have
an aversion to mocking: every time you alter the test environment, you run
the risk that those differences from reality will mask changes seen in
the real environment.</p>
<p>So, it makes sense that a good first goal for creating a test suite against
our Mercurial server should be to reproduce the production server and
environment as closely as possible.</p>
<p>I'm currently working on a Vagrant environment that attempts to
reproduce the official environment as closely as possible. It starts
one virtual machine for the SSH/master server. It starts a separate
virtual machine for the hgweb/slave servers. The virtual machines are
booting CentOS. This is different than production, where we run RHEL.
But they are similar enough (and can share the same packages) that the
differences shouldn't matter too much, at least for now.</p>
<h2>Using Puppet</h2>
<p>In production, Mozilla is using Puppet to manage the Mercurial servers.
Unfortunately, the actual Puppet configs that Mozilla is running are
behind a firewall, mainly for security reasons. This is potentially
a huge setback for my reproducibility effort, as I'd like to have
my virtual machines use the same exact Puppet configs as whats used
in production so the environments match as closely as possible. This
would also save me a lot of work from having to reinvent the wheel.</p>
<p>Fortunately, Ben Kero has extracted the Mercurial-relevant Puppet
config files into a
<a href="https://github.com/bkero/puppet-module-hg">standalone repository</a>.
Apparently that repository gets rolled into the production Puppet
configs periodically. So, my virtual machines and production can share
the same Mercurial Puppet files. Nice!</p>
<p>It wasn't long after starting to use the standalone Puppet configs that
I realized this would be a rabbit hole. This first manifests in the
standalone Puppet code referencing things that exist in the hidden
Mozilla Puppet files. So the liberation was only partially successful.
Sad panda.</p>
<p>So, I'm now in the process of creating a <em>fake Mozilla</em> Puppet
environment that mimics the base Mozilla environment (from the closed
repo) and am modifying the shared Puppet Mercurial code to work with
both versions. This is a royal pain, but it needs to be done if we want
to reproduce production and maintain peace of mind that test results
reflect reality.</p>
<p>Because reproducing runtime environments is important for reproducing
and solving bugs and for testing, <strong>I call on the maintainers of
Mozilla's closed Puppet repository to liberate it from behind its
firewall</strong>. I'd like to see a public Puppet configuration tree available
for all to use so that anyone anywhere can reproduce the state of a
server or service operated by Mozilla to within reasonable
approximation. Had this already been done, it would have saved me hours
of work. As it stands, I'm reverse engineering systems and trying to
cobble together understanding of how the Mozilla Puppet configs
work and what parts of them can safely be ignored to reproduce an
approximate testing environment.</p>
<p>Along that vein, I finally got access to Mozilla's internal Puppet
repository. This took a few meetings and apparently a lot of
backroom chatter was generated - "developer's don't normally get access,
oh my!" All I wanted was to see how systems are configured so I can
help improve them. Instead, getting access felt like pulling teeth.
This feels like a major roadblock towards productivity, reproducibility,
and testing.</p>
<p>Facebook gives its developers access to most production machines and
trusts them to not be stupid. I know we (Mozilla) like to hold
ourselves to a high standard of security and privacy. But not giving
developers access to the configurations for the systems their code
runs on feels like a very silly policy. I hope Mozilla invests in
opening up this important code and data, if not to the world, at least
to its trusted employees.</p>
<p>Anyway, hopefully I'll soon have a Vagrant environment that allows
people to build a standalone instance of Mozilla's Mercurial server.
And once that's in place, I can start writing tests that basic services
and workflows (including repository syncing) work as expected. Stay
tuned.</p>]]></content:encoded>
    </item>
    <item>
      <title>New Job Role</title>
      <link>http://gregoryszorc.com/blog/2014/09/05/new-job-role</link>
      <pubDate>Fri, 05 Sep 2014 11:30:00 PDT</pubDate>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/09/05/new-job-role</guid>
      <description>New Job Role</description>
      <content:encoded><![CDATA[<p>As of today, I have a new role and title at Mozilla: <em>Developer
Productivity Engineer</em>. I'll be reporting to Laura Thomson as a
member of the <em>Developer Services</em> team.</p>
<p>I have an immediate goal to make our version control work better.
This includes making Try scale and helping out with the deployment
of ReviewBoard. After that, I'm not entirely sure. But Autoland
and Firefox build system improvements have been discussed.</p>
<p>I'm really excited to be in this new role. If someone were to give me
a clean slate and tell me to design my own job role, I think I'd
answer with something very similar to the role I am now in. I am
passionate about tools and enabling people to become more productive.
I have little doubt I'll thrive in this new role.</p>]]></content:encoded>
    </item>
    <item>
      <title>Submit Feedback about Mercurial</title>
      <link>http://gregoryszorc.com/blog/2014/08/19/submit-feedback-about-mercurial</link>
      <pubDate>Tue, 19 Aug 2014 18:30:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/08/19/submit-feedback-about-mercurial</guid>
      <description>Submit Feedback about Mercurial</description>
      <content:encoded><![CDATA[<p>Are you a Mozillian who uses Mercurial? Do you have a complaint,
suggestion, observation, or any other type of feedback you'd like
to give to the maintainers of Mercurial? Now's your chance.</p>
<p>There is a large gathering of Mercurial contributors next weekend
in Munich. The
<a href="http://mercurial.selenic.com/wiki/3.2sprint#Possible_Topics">topics list</a>
is already impressive. But Mozilla's <em>delegation</em> (Mike Hommey,
Ben Kero, and myself) would love to advance Mozilla's concerns to
the wider community.</p>
<p>To leave or vote for feedback, please visit
<a href="https://hgfeedback.paas.allizom.org/e/august-2014-summit">https://hgfeedback.paas.allizom.org/e/august-2014-summit</a>
before August 29 so your voice may be heard.</p>
<p>I encourage you to leave feedback about any small, big or small,
Mozilla-specific or not. Comparisons to Git, GitHub and other
version control tools and services are also welcome.</p>
<p>If you have feedback that can't be captured in that moderator tool,
please email me. gps@mozilla.com.</p>]]></content:encoded>
    </item>
    <item>
      <title>Mercurial hooks move and testing Mercurial</title>
      <link>http://gregoryszorc.com/blog/2014/08/18/mercurial-hooks-move-and-testing-mercurial</link>
      <pubDate>Mon, 18 Aug 2014 15:10:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/08/18/mercurial-hooks-move-and-testing-mercurial</guid>
      <description>Mercurial hooks move and testing Mercurial</description>
      <content:encoded><![CDATA[<p>Mozilla has a number of source repositories under
<a href="https://hg.mozilla.org/hgcustom/">https://hg.mozilla.org/hgcustom/</a>
that cumulatively define how version control works at Mozilla.</p>
<p>Back in February, I
<a href="/blog/2014/02/05/new-repository-for-mozilla-version-control-tools/">launched an effort</a>
to establish a unified Mercurial repository for all this code. That
repository is <a href="https://hg.mozilla.org/hgcustom/version-control-tools/">version-control-tools</a>
and it has slowly grown.</p>
<p>The <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1052201">latest addition</a>
to this repository is the import of the <em>hghooks</em> repository. This
now-defunct repository contained all the server-side Mercurial hooks
that Mozilla has deployed on <em>hg.mozilla.org</em>.</p>
<p>Soon after that repository was imported into version-control-tools, we
started executing the hooks tests as part of the existing test suite in
version-control-tools. This means we get
<a href="https://ci.mozilla.org/job/version-control-tools/">continuous integration</a>,
<a href="https://ci.mozilla.org/job/version-control-tools/coveragepy/?">code coverage</a>,
and the ability to run tests against multiple versions of Mercurial
(2.5.4 through 3.1) in one go.</p>
<p><strong>This is new for Mozilla and is a big deal.</strong> For the first time, we
have a somewhat robust testing environment for Mercurial that is testing
things we run in production.</p>
<p>But we still have a long way to go. The ultimate goal is to get everything
rolled into the version-control-tools repository and to write tests for
everything people rely on. We also want the test environment to look as
much like our production environment as possible. Once that's in place,
most of the fear and uncertainty around upgrading or changing the server
goes away. This will allow Mozilla to move faster and issues like our
recent server problems can be diagnosed more quickly (Mercurial has
added better logging in newer versions).</p>
<p>If you want to contribute to this effort, <strong>please write tests for
behavior you rely on.</strong> We're now relying on Mercurial's test harness
and <a href="http://mercurial.selenic.com/wiki/WritingTests">test types</a>
rather than low-level unit tests. This means our tests are now running
a Mercurial server and running actual Mercurial commands. The tests thus
explicitly verify that client-seen behavior is exactly as you intend.
For an example, see the
<a href="https://hg.mozilla.org/hgcustom/version-control-tools/file/abc1c5bdca9b/hghooks/tests/test-prevent-webidl.t">WebIDL hook test</a>.</p>
<p>So what are you waiting for? Find some gaps in code coverage and write
some tests today!</p>]]></content:encoded>
    </item>
    <item>
      <title>Please run mach mercurial-setup</title>
      <link>http://gregoryszorc.com/blog/2014/07/25/please-run-mach-mercurial-setup</link>
      <pubDate>Fri, 25 Jul 2014 10:00:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/07/25/please-run-mach-mercurial-setup</guid>
      <description>Please run mach mercurial-setup</description>
      <content:encoded><![CDATA[<p>Hey there, Firefox developer! Do you use Mercurial? Please take the time
right now to run <strong>mach mercurial-setup</strong> from your Firefox clone.</p>
<p>It's been updated to ensure you are running a modern Mercurial version.
More awesomely, it has support for a couple of new extensions to make
you more productive. I think you'll like what you see.</p>
<p><strong>mach mercurial-setup</strong> doesn't change your <em>hgrc</em> without
confirmation. So it is safe to run to see what's available. You should
consider running it periodically, say once a week or so. I wouldn't
be surprised if we add a notification to mach to remind you to do this.</p>]]></content:encoded>
    </item>
    <item>
      <title>Repository-Centric Development</title>
      <link>http://gregoryszorc.com/blog/2014/07/24/repository-centric-development</link>
      <pubDate>Thu, 24 Jul 2014 20:23:00 PDT</pubDate>
      <category><![CDATA[Git]]></category>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/07/24/repository-centric-development</guid>
      <description>Repository-Centric Development</description>
      <content:encoded><![CDATA[<p>I was editing a wiki page yesterday and I think I coined a new term
which I'd like to enter the common nomenclature: <em>repository-centric
development</em>. The term refers to development/version control
workflows that place repositories - not patches - first.</p>
<p>When collaborating on version controlled code with modern tools like
Git and Mercurial, you essentially have two choices on how to
share version control data: patches or repositories.</p>
<p>Patches have been around since the dawn of version control. Everyone
knows how they work: your version control system has a copy of
the canonical data and it can export a view of a specific change
into what's called a patch. A patch is essentially a diff with
extra metadata.</p>
<p>When distributed version control systems came along, they brought
with them an alternative to patch-centric development:
repository-centric development. You could still exchange patches if you
wanted, but distributed version control allowed you to <em>pull</em> changes
directly from multiple <em>repositories</em>. You weren't limited to a single
master server (that's what the <em>distributed</em> in <em>distributed version
control</em> means). You also didn't have to go through an intermediate
transport such as email to exchange patches: you communicate directly
with a peer repository instance.</p>
<p>Repository-centric development eliminates the <em>middle man</em> required
for patch exchange: instead of exchanging derived data, you exchange
the actual data, speaking the repository's native language.</p>
<p>One advantage of repository-centric development is it eliminates the
problem of patch non-uniformity. Patches come in many different flavors.
You have plain diffs. You have diffs with metadata. You have Git style
metadata. You have Mercurial style metadata. You can produce patches
with various lines of context in the diff. There are different methods
for handling binary content. There are different ways to express
file adds, removals, and renames. It's all a hot mess. Any system
that consumes patches needs to deal with the non-uniformity. Do you
think this isn't a problem in the real world? Think again. If you are
involved with an open source project that collects patches via email
or by uploading patches to a bug tracker, have you ever seen someone
accidentally upload a patch in the wrong format? That's patch
non-uniformity. New contributors to Firefox do this all the time. I
also see it in the Mercurial project. With repository-centric
development, patches never enter the picture, so patch non-uniformity
is a non-issue. (Don't confuse the superficial formatting of patches
with the content, such as an incorrect commit message format.)</p>
<p>Another advantage of repository-centric development is it makes the
act of exchanging data easier. Just have two repositories talk to
each other. This used to be difficult, but hosting services like
GitHub and Bitbucket make this easy. Contrast with patches, which
require hooking your version control tool up to wherever those patches
are located. The Linux Kernel, like so many other projects,
<a href="https://www.kernel.org/doc/Documentation/SubmittingPatches">uses email for contributing changes</a>.
So now Git, Mercurial, etc all fulfill Zawinski's law. This means your
version control tool is talking to your inbox to send and receive code.
Firefox development uses Bugzilla to hold patches as attachments. So now your
version control tool needs to talk to your issue tracker. (Not the worst
idea in the world I will concede.) While, yes, the tools around using
email or uploading patches to issue trackers or whatever else you are
using to exchange patches exist and can work pretty well, the grim
reality is that these tools are all reinventing the wheel of repository
exchange and are solving a problem that has already been solved by
<em>git push</em>, <em>git fetch</em>, <em>hg pull</em>, <em>hg push</em>, etc. Personally, I would
rather <em>hg push</em> to a remote and have tools like issue trackers and
mailing lists pull directly from repositories. At least that way they
have a direct line into the source of truth and are guaranteed a
consistent output format.</p>
<p>Another area where direct exchange is huge is multi-patch commits
(<em>branches</em> in Git parlance) or where commit data is fragmented. When
pushing patches to email, you need to insert metadata saying which patch
comes after which. Then the email import tool needs to reassemble things
in the proper order (remember that the typical convention is one email
per patch and email can be delivered out of order). Not the most
difficult problem in the world to solve. But seriously, it's been
solved already by <em>git fetch</em> and <em>hg pull</em>! Things are worse for
Bugzilla. There is no bullet-proof way to order patches there. The
convention at Mozilla is to add <em>Part N</em> strings to
commit messages and have the Bugzilla import tool do a sort (I assume it
does that). But what if you have a logical commit series spread across
multiple bugs? How do you reassemble everything into a linear series of
commits? You don't, sadly. Just today I wanted to apply a somewhat
complicated series of patches to the Firefox build system I was asked to
review so I could jump into a debugger and see what was going on so I
could conduct a more thorough review. There were 4 or 5 patches spread
over 3 or 4 bugs. Bugzilla and its patch-centric workflow prevented me
from importing the patches. Fortunately, this patch series was pushed to
Mozilla's Try server, so I could pull from there. But I haven't always
been so fortunate. This limitation means developers have to make
sacrifices such as writing fewer, larger patches (this makes code review
harder) or involving unrelated parties in the same bug and/or review.
In other words, deficient tools are imposing limited workflows. No bueno.</p>
<p>It is a fair criticism to say that not everyone can host a server or
that permissions and authorization are hard. Although I think concerns
about impact are overblown. If you are a small project, just create a
GitHub or Bitbucket account. If you are a larger project, realize that
people time is one of your largest expenses and invest in tools like
proper and efficient repository hosting (often this can be GitHub) to
reduce this waste and keep your developers happier and more efficient.</p>
<p>One of the clearest examples of repository-centric development is
GitHub. There are no patches in GitHub. Instead, you <em>git push</em>
and <em>git fetch</em>. Want to apply someone else's work? Just add a remote
and <em>git fetch</em>! Contrast with first locating patches, hooking up
Git to consume them (this part was always confusing to me - do you
need to retroactively have them sent to your email inbox so you can
import them from there), and finally actually importing them. Just
give me a URL to a repository already. But the benefits of
repository-centric development with GitHub don't stop at pushing and
pulling. GitHub has built code review functionality into pushes. They
call these <em>pull requests</em>. While I have significant issues with
GitHub's implemention of pull requests (I need to blog about those some
day), I can't deny the utility of the repository-centric workflow and
all the benefits around it. Once you switch to GitHub and its
repository-centric workflow, you more clearly see how lacking
patch-centric development is and quickly lose your desire to go back
to the 1990's state-of-the-art methods for software development.</p>
<p>I hope you now know what repository-centric development is and will join
me in championing it over patch-based development.</p>
<p>Mozillians reading this will be very happy to learn that work is under
way to shift Firefox's development workflow to a more repository-centric
world. Stay tuned.</p>]]></content:encoded>
    </item>
    <item>
      <title>Updates to firefoxtree Mercurial extension</title>
      <link>http://gregoryszorc.com/blog/2014/07/16/updates-to-firefoxtree-mercurial-extension</link>
      <pubDate>Wed, 16 Jul 2014 19:55:00 PDT</pubDate>
      <category><![CDATA[Mercurial]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/07/16/updates-to-firefoxtree-mercurial-extension</guid>
      <description>Updates to firefoxtree Mercurial extension</description>
      <content:encoded><![CDATA[<p>My <a href="/blog/2014/06/23/please-stop-using-mq/">Please Stop Using MQ</a> post,
has been generating a lot of interest for bookmark-based workflows at
Mozilla. To make adoption easier, I
<a href="/blog/2014/06/30/track-firefox-repositories-with-local-only-mercurial-tags/">quickly authored</a>
an <a href="https://hg.mozilla.org/hgcustom/version-control-tools/file/default/hgext/firefoxtree/__init__.py">extension</a>
to add <em>remote refs</em> of Firefox repositories to Mercurial.</p>
<p>There was still a bit of confusion and gripes about workflows that I
thought it would be best to update the extension to make things more
pleasant.</p>
<h2>Automatic tree names</h2>
<p>People wanted an ability to easy pull/aggregate the various Firefox
trees without additional configuration to an hgrc file.</p>
<p>With <em>firefoxtree</em>, you can now <em>hg pull central</em> or <em>hg pull inbound</em>
or <em>hg pull aurora</em> and it just works.</p>
<p>Pushing with aliases doesn't yet work. It is slightly harder to do in
the Mercurial API. I have a solution, but I'm validating some code paths
to ensure it is safe. This feature will likely appear soon.</p>
<h2>fxheads commands</h2>
<p>Once people adopted unified repositories with heads from multiple
repositories, they asked how they could quickly identify the heads of
the pulled Firefox repositories.</p>
<p><em>firefoxtree</em> now provides a <em>hg fxheads</em> command that prints a
concise output of the commits constituting the heads of the Firefox
repos. e.g.</p>
<div class="pygments_murphy"><pre>$ hg fxheads
224969:0ec0b9ac39f0 aurora (sort of) bug 898554 - raise expected hazard count for b2g to 4 until they are fixed, a=bustage+hazbuild-only
224290:6befadcaa685 beta Tagging /src/mdauto/build/mozilla-beta 1772e55568e4 with FIREFOX_RELEASE_31_BASE a=release CLOSED TREE
224848:8e8f3ba64655 central Merge inbound to m-c a=merge
225035:ec7f2245280c fx-team fx-team/default Merge m-c to fx-team
224877:63c52b7ddc28 inbound Bug 1039197 - Always build js engine with zlib. r=luke
225044:1560f67f4f93 release release/default tip Automated checkin: version bump for firefox 31.0 release. DONTBUILD CLOSED TREE a=release
</pre></div>

<p>Please note that the output is based upon local-only knowledge: you'll
need to pull to ensure data is current.</p>
<h2>Reject pushing multiple heads</h2>
<p>People were complaining that bookmark-based workflows resulted in
Mercurial trying to push multiple heads to a remote. This complaint
stems from the fact that Mercurial's default push behavior is to find
all commits missing from the remote and push them. This behavior is
extremely frustrating for Firefox development because the Firefox repos
only have a single head and pushing multiple heads will only result in
a server hook rejecting the push (after wasting a lot of time
transferring that commit data).</p>
<p><em>firefoxtree</em> now will refuse to push multiple heads to a known Firefox
repo before any commit data is sent. In other words, we fail fast so
your time is saved.</p>
<p><em>firefoxtree</em> also changes the default behavior of <em>hg push</em> when
pushing to a Firefox repo. If no <em>-r</em> argument is specified, <em>hg push</em>
to a Firefox repo will automatically remap to <em>hg push -r .</em>. In other
words, we attempt to push the working copy's commit by default. This
change establishes sensible default and likely working behavior when
typing just <em>hg push</em>.</p>
<p>I am a bit on the fence about changing the default behavior of <em>hg
push</em>. On one hand, it makes total sense. On the other, silently
changing the default behavior of a built-in command is a little
dangerous. I can easily see this backfiring when people interact with
non-Firefox repos. I encourage people to get in the habit of typing
<em>hg push -r <rev></em> because that's what you should be doing.</p>
<h2>Installing firefoxtree</h2>
<p>Within the next 48 hours, <em>mach mercurial-setup</em> should prompt to
install <em>firefoxtree</em>. Until then, clone
<em>https://hg.mozilla.org/hgcustom/version-control-tools</em> and ensure your
<em>~/.hgrc</em> file has the following:</p>
<pre><code>[extensions]
firefoxtree = /path/to/version-control-tools/hgext/firefoxtree
</code></pre>
<p>You likely already have a copy of version-control-tools
in <em>~/.mozbuild/version-control-tools</em>.</p>
<p>It is completely safe to install <em>firefoxtree</em> globally: the extension
will only modify behavior of repositories that are clones of Firefox
repositories.</p>]]></content:encoded>
    </item>
    <item>
      <title>Python Packaging Do's and Don'ts</title>
      <link>http://gregoryszorc.com/blog/2014/07/15/python-packaging-do's-and-don'ts</link>
      <pubDate>Tue, 15 Jul 2014 17:20:00 PDT</pubDate>
      <category><![CDATA[Python]]></category>
      <category><![CDATA[Mozilla]]></category>
      <guid isPermaLink="true">http://gregoryszorc.com/blog/2014/07/15/python-packaging-do's-and-don'ts</guid>
      <description>Python Packaging Do's and Don'ts</description>
      <content:encoded><![CDATA[<p>Are you someone who casually interacts with Python but don't know the
inner workings of Python? Then this post is for you. Read on to learn
why some things are the way they are and how to avoid making some
common mistakes.</p>
<h2>Always use Virtualenvs</h2>
<p>It is an easy trap to view <a href="https://virtualenv.pypa.io/en/latest/virtualenv.html">virtualenvs</a>
as an obstacle, a distraction towards accomplishing something. People
see me adding virtualenvs to build instructions and they say <em>I don't
use virtualenvs, they aren't necessary, why are you doing that?</em></p>
<p>A virtualenv is effectively an overlay on top of your system Python
install. Creating a virtualenv can be thought of as copying your system
Python environment into a local location. When you modify virtualenvs,
you are modifying an isolated container. Modifying virtualenvs has no
impact on your system Python.</p>
<p>A goal of a virtualenv is to isolate your system/global Python install
from unwanted changes. When you accidentally make a change to a
virtualenv, you can just delete the virtualenv and start over from
scratch. When you accidentally make a change to your system Python, it
can be much, much harder to recover from that.</p>
<p>Another goal of virtualenvs is to allow different versions of packages
to exist. Say you are working on two different projects and each
requires a specific version of Django. With virtualenvs, you install one
version in one virtualenv and a different version in another virtualenv.
Things happily coexist because the virtualenvs are independent.
Contrast with trying to manage both versions of Django in your system
Python installation. Trust me, it's not fun.</p>
<p>Casual Python users may not encounter scenarios where virtualenvs make
their lives better... until they do, at which point they realize their
system Python install is beyond saving. People who eat, breath, and die
Python run into these scenarios all the time. We've learned how bad life
without virtualenvs can be and so we use them everywhere.</p>
<p>Use of virtualenvs is a best practice. Not using virtualenvs will result
in something unexpected happening. It's only a matter of time.</p>
<p>Please use virtualenvs.</p>
<h2>Never use sudo</h2>
<p>Do you use sudo to install a Python package? You are doing it wrong.</p>
<p>If you need to use sudo to install a Python package, that almost
certainly means you are installing a Python package to your
system/global Python install. And this means you are modifying your
system Python instead of isolating it and keeping it pristine.</p>
<p>Instead of using sudo to install packages, create a virtualenv and
install things into the virtualenv. There should never be permissions
issues with virtualenvs - the user that creates a virtualenv has full
realm over it.</p>
<h2>Never modify the system Python environment</h2>
<p>On some systems, such as OS X with Homebrew, you don't need sudo to
install Python packages because the user has write access to the Python
directory (<em>/usr/local</em> in Homebrew).</p>
<p>For the reasons given above, don't muck around with the system Python
environment. Instead, use a virtualenv.</p>
<h2>Beware of the package manager</h2>
<p>Your system's package manager (apt, yum, etc) is likely using root and/or
installing Python packages into the system Python.</p>
<p>For the reasons given above, this is bad. Try to use a virtualenv, if
possible. Try to not use the system package manager for installing
Python packages.</p>
<h2>Use pip for installing packages</h2>
<p>Python packaging has historically been a mess. There are a handful of
tools and APIs for installing Python packages. As a casual Python user,
you only need to know of one of them:
<a href="https://pip.pypa.io/en/latest/">pip</a>.</p>
<p>If someone says <em>install a package</em>, you should be thinking <em>create a
virtualenv, activate a virtualenv, <code>pip install &lt;package&gt;</code></em>. <strong>You
should never run <code>pip install</code> outside of a virtualenv.</strong> (The exception
is to install virtualenv and pip itself, which you almost certainly want
in your system/global Python.)</p>
<p>Running <em>pip install <package></em> will install packages from
<a href="https://pypi.python.org/pypi">PyPI</a>, the Python Packaging Index by
default. It's Python's official package repository.</p>
<p>There are a lot of old and outdated tutorials online about Python
packaging. Beware of bad content. For example, if you see documentation
that says <em>use easy_install</em>, you should be thinking, <em>easy_install is
a legacy package installer that has largely been replaced by pip, I
should use pip instead</em>. When in doubt, consult the
<a href="https://python-packaging-user-guide.readthedocs.org/en/latest/index.html">Python packaging user guide</a>
and do what it recommends.</p>
<h2>Don't trust the Python in your package manager</h2>
<p>The more Python programming you do, the more you learn to not trust the
Python package provided by your system / package manager.</p>
<p>Linux distributions such as Ubuntu that sit on the forward edge of
versions are better than others. But I've run into enough problems with
the OS or package manager maintained Python (especially on OS X), that
I've learned to distrust them.</p>
<p>I use <a href="https://github.com/yyuu/pyenv">pyenv</a> for installing and managing
Python distributions from source. pyenv also installs virtualenv and
pip for me, packages that I believe should be in all Python installs
by default. As a more experienced Python programmer, I find pyenv 
<em>just works</em>.</p>
<p>If you are just a beginner with Python, it is probably safe to ignore
this section. Just know that as soon as something weird happens, start
suspecting your default Python install, especially if you are on OS X.
If you suspect trouble, use something like pyenv to enforce a buffer so
the system can have its Python and you can have yours.</p>
<h2>Recovering from the past</h2>
<p>Now that you know the preferred way to interact with Python, you are
probably thinking <em>oh crap, I've been wrong all these years - how do I
fix it?</em></p>
<p>The goal is to get a Python install <em>somewhere</em> that is as pristine as
possible. You have two approaches here: cleaning your existing Python or
creating a new Python install.</p>
<p>To clean your existing Python, you'll want to purge it of pretty much
all packages not installed by the core Python distribution. The
exception is virtualenv, pip, and setuptools - you almost certainly want
those installed globally. On Homebrew, you can uninstall everything related to
Python and blow away your Python directory, typically
/usr/local/lib/python*. Then, <em>brew install python</em>. On Linux distros,
this is a bit harder, especially since most Linux distros rely on Python
for OS features and thus they may have installed extra packages. You
could try a similar approach on Linux, but I don't think it's worth it.</p>
<p>Cleaning your system Python and attempting to keep it pure are ongoing
tasks that are very difficult to keep up with. All it takes is one
dependency to get pulled in that trashes your system Python. Therefore,
I shy away from this approach.</p>
<p>Instead, I install and run Python from my user directory. I use
<a href="https://github.com/yyuu/pyenv">pyenv</a>. I've also heard great things
about <a href="http://conda.pydata.org/miniconda.html">Miniconda</a>. With either
solution, you get a Python in your home directory that starts clean and
pure. Even better, it is completely independent from your system Python.
So if your package manager does something funky, there is a buffer. And,
if things go wrong with your userland Python install, you can always
nuke it without fear of breaking something in system land. This seems to
be the best of both worlds.</p>
<p>Please note that installing packages in the system Python shouldn't be
evil. When you create virtualenvs, you can - and should - tell
virtualenv to not use the system site-packages (i.e. <em>don't use
non-core packages from the system installation</em>). This is the default
behavior in virtualenv. It should provide an adequate buffer. But from
my experience, things still manage to bleed through. My userland Python
install is extra safety. If something wrong happens, I can only blame
myself.</p>
<h2>Conclusion</h2>
<p>Python's long and complicated history of package management makes it
very easy for you to shoot yourself in the foot. The long list of
outdated tutorials on The Internet make this a near certainty for casual
Python users. Using the guidelines in this post, you can adhere to best
practices that will cut down on surprises and rage and keep your Python
running smoothly.</p>]]></content:encoded>
    </item>
  </channel>
</rss>
