<?xml version="1.0" encoding="UTF-8"?>
<feed
  xmlns="http://www.w3.org/2005/Atom"
  xmlns:thr="http://purl.org/syndication/thread/1.0"
  xml:lang="en"
   >
  <title type="text">Gregory Szorc's Digital Home</title>
  <subtitle type="text">Rambling on</subtitle>

  <updated>2015-01-09T23:29:33Z</updated>
  <generator uri="http://blogofile.com/">Blogofile</generator>

  <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog" />
  <id>http://gregoryszorc.com/blog/feed/atom/</id>
  <link rel="self" type="application/atom+xml" href="http://gregoryszorc.com/blog/feed/atom/" />
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Style Changes on hg.mozilla.org]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2015/01/09/style-changes-on-hg.mozilla.org" />
    <id>http://gregoryszorc.com/blog/2015/01/09/style-changes-on-hg.mozilla.org</id>
    <updated>2015-01-09T15:25:00Z</updated>
    <published>2015-01-09T15:25:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Style Changes on hg.mozilla.org]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2015/01/09/style-changes-on-hg.mozilla.org"><![CDATA[<p>Starting today and continuing through next week, there will be a number
of styling changes made to <a href="https://hg.mozilla.org/">hg.mozilla.org</a>.</p>
<p>The main goal of the work is to bring the style up-to-date with upstream
Mercurial. This will result in more features being available to the web
interface, hopefully making it more useful. This includes display of
bookmarks and the Mercurial help documentation. As part of this work,
we're also removing some files on the server that shouldn't be used. If
you start getting 404s or notice an unexpected theme change, this is
probably the reason why.</p>
<p>If you'd like to look over the changes before they are made or would
like to file a bug against a regression (we suspect there will be
minor regressions due to the large nature of the changes), head on
over to <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1117021">bug 1117021</a>
or ping people in #vcs on IRC.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Mercurial Pushlog Is Now Robust Against Interrupts]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/12/30/mercurial-pushlog-is-now-robust-against-interrupts" />
    <id>http://gregoryszorc.com/blog/2014/12/30/mercurial-pushlog-is-now-robust-against-interrupts</id>
    <updated>2014-12-30T12:25:00Z</updated>
    <published>2014-12-30T12:25:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <category scheme="http://gregoryszorc.com/blog" term="Firefox" />
    <summary type="html"><![CDATA[Mercurial Pushlog Is Now Robust Against Interrupts]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/12/30/mercurial-pushlog-is-now-robust-against-interrupts"><![CDATA[<p><a href="https://hg.mozilla.org">hg.mozilla.org</a> - Mozilla's Mercurial server -
has functionality called the <a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/hgmo/pushlog.html">pushlog</a>
which records who pushed what when. Essentially, it's a log of when
a repository was changed. This is separate from the commit log because
the commit log can be spoofed and the commit log doesn't record when
commits were actually pushed.</p>
<p>Since its inception, the pushlog has suffered from data consistency
issues. If you aborted the push at a certain time, data was not inserted
in the pushlog. If you aborted the push at another time, data existed in
the pushlog but not in the repository (the repository would get rolled
back but the pushlog data wouldn't).</p>
<p><strong>I'm pleased to announce that the pushlog is now robust against
interruptions and its updates are consistent with what is recorded by
Mercurial.</strong> The pushlog database commit/rollback is tied to Mercurial's
own transaction API. What Mercurial does to the push transaction, the
pushlog follows.</p>
<p>This former inconsistency has caused numerous problems over the years.
When data was inconsistent, we often had to close trees until someone
could SSH into the machines and manually run SQL to fix the problems.
This also contributed to a culture of <em>don't press ctrl+c during push:
it could corrupt Mercurial.</em> (Ctrl+c should be safe to press any time: if
it isn't, there is a bug to be filed.)</p>
<p>Any time you remove a source of tree closures is a cause for celebration.
Please join me in celebrating your new freedom to abort pushes without
concern for data inconsistency.</p>
<p>In case you want to test things out, aborting pushes and (and rolling
back the pushlog) should now result in something like:</p>
<pre><code>pushing to ssh://hg.mozilla.org/mozilla-central
searching for changes
adding changesets
adding manifests
adding file changes
added 1 changesets with 1 changes to 1 files
Trying to insert into pushlog.
Inserted into the pushlog db successfully.
^C
rolling back pushlog
transaction abort!
rollback completed
</code></pre>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Firefox Source Documentation Versus MDN]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/12/30/firefox-source-documentation-versus-mdn" />
    <id>http://gregoryszorc.com/blog/2014/12/30/firefox-source-documentation-versus-mdn</id>
    <updated>2014-12-30T12:00:00Z</updated>
    <published>2014-12-30T12:00:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Firefox Source Documentation Versus MDN]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/12/30/firefox-source-documentation-versus-mdn"><![CDATA[<p>The Firefox source tree has had in-tree documentation powered by
<a href="https://gecko.readthedocs.org/en/latest/">Sphinx</a> for a while now.
However, its canonical home has been a
<a href="https://ci.mozilla.org/job/mozilla-central-docs/Tree_Documentation/index.html">hard-to-find URL on ci.mozilla.org</a>.
I finally <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1115278">scratched an itch</a>
and wrote patches to enable the docs to be built easier. So,
starting today, the docs are now available on Read the Docs at
<a href="https://gecko.readthedocs.org/en/latest/">https://gecko.readthedocs.org/en/latest/</a>!</p>
<p>While I was scratching itches, I decided to play around with another
documentation-related task: automatic API documentation. I have a
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1115452">limited proof-of-concept for automatically generating XPIDL interface
documentation</a>.
Essentially, we use the in-tree XPIDL parser to parse <em>.idl</em> files and
turn the Python object representation into reStructured Text, which Sphinx
parses and renders into pretty HTML for us. The concept can be applied to any
source input, such as WebIDL and JavaScript code. I chose XPIDL because a
parser is readily available and I know Joshua Cranmer has expressed
interest in automatic XPIDL documentation generation. (As an aside,
JavaScript tooling that supports the flavor of JavaScript used
internally by Firefox is very limited. We need to prioritize removing
Mozilla extensions to JavaScript if we ever want to start using awesome
tooling that exists in the wild.)</p>
<p>As I was implementing this proof-of-concept, I was looking at XPIDL
interface documentation on MDN to see how things are presented today.
After perusing MDN for a bit and comparing its content against what I
was able to derive from the source, something became extremely clear:
MDN has significantly more content than the canonical source code.
Obviously the <em>.idl</em> files document the interfaces, their attributes,
their methods, and all the types and names in between: that's the very
definition of an IDL. But what was generally missing from the source
code is comments. <em>What does this method do?</em> <em>What is each argument
used for?</em> Things like example usage are almost non-existent in the
source code. MDN, by contrast, typically has no shortage of all these
things.</p>
<p>As I was grasping the reality that MDN has a lot of out-of-tree
supplemental content, I started asking myself <em>what's the point
in automatic API docs? Is manual document curation on MDN good
enough?</em> <strong>This question has sort of been tearing me apart.</strong> Let me try
to explain.</p>
<p>MDN is an amazing site. You can tell a lot of love has gone into making
the experience and much of its content excellent. However, the content
around the technical implementation / internals of Gecko/Firefox generally
sucks. There are some exceptions to the rule. But I find that things like
internal API documentation to be lackluster on average. It is rare for
me to find documentation that is up-to-date and useful. It is common to
find documentation that is partial and incomplete. It is very common to
find things like JSMs not documented at all. <strong>I think this is a
problem.</strong> I argue the lack of good documentation raises the barrier to
contributing. <strong>Furthermore, writing and maintaining excellent
low-level documentation is too much effort.</strong></p>
<p><strong>My current thoughts on API and low-level documentation are that I
question the value of this documentation existing on MDN.</strong>
Specifically, I think things like JSM API docs
(like <a href="https://developer.mozilla.org/en-US/docs/Mozilla/JavaScript_code_modules/Sqlite.jsm">Sqlite.jsm</a>)
and XPIDL interface documentation (like
<a href="https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XPCOM/Reference/Interface/nsIFile">nsIFile</a>)
don't belong on MDN - at least not in wiki form. <strong>Instead, I believe
that documentation like this should live in and be derived from source
code.</strong> Now, if the MDN site wants to expose this as read-only content
or if MDN wants to enable the content to be annotated in a wiki-like
manner (like how MSDN and PHP documentation allow user comments), that's
perfectly fine by me. Here's why.</p>
<p>First, if I must write separate-from-source-code API documentation on
MDN (or any other platform for that matter), I must now perform extra
work or forgo either the source code or external documentation. In other
words, if I write in-line documentation in the source code, I must spend
extra effort to essentially copy large parts of that to MDN. And I must
continue to spend extra effort to keep updates in sync. If I don't
want to spend that extra effort (I'm as lazy as you), I have to choose
between documenting the source code or documenting MDN. If I choose the
source code, people either have to read the source to read the docs
(because we don't generate documentation from source today) or someone
else has to duplicate the docs (overall more work). If I choose to
document on MDN, then people reading the source code (probably because
they want to change it) are deprived of additional context useful to
make that process easier. <strong>This is a lose-lose scenario and it is a
general waste of expensive people time.</strong></p>
<p>Second, I prefer having API documentation derived from source code
because I feel it results in more accurate documentation that has the
higher liklihood of remaining accurate and in sync with reality. Think
about it: when was the last time you reviewed changes to a JSM and
searched MDN for content that needed updated? I'm sure there are some
pockets of people that do this right. But I've written dozens of
JavaScript patches for Firefox and I'm pretty sure I've been asked to
update <em>external</em> documentation less than 5% of the time. Inline source
documentation, however, is another matter entirely. Because the
documentation is often proximal to code that changed, I frequently a) go
ahead and make the documentation changes because everything is right there
and it's low overhead to change as I adjust the source b) am asked to
update in-line docs when a reviewer sees I forgot to. Generally
speaking, things tend to stay in sync and fewer bugs form when
everything is proximally located. By fragmenting documentation between
source code and external services like MDN, we increase the liklihood
that things become out of sync. This results in misleading information
and increases the barriers to contribution and change. In other words,
developer inefficiency.</p>
<p>Third, having API documentation derived from source code opens up
numerous possibilities to further aid developer productivity and improve
the usefullness of documentation. For example:</p>
<ul>
<li>We can parse <em>@param</em> references out of documentation and issue
  warnings/errors when documentation doesn't match the AST.</li>
<li>We can issue warnings when code isn't documented.</li>
<li>We can include in-line examples and execute and verify these as part
  of builds/tests.</li>
<li>We can more easily cross-reference APIs because everything is
  documented consistently. We can also issue warnings when
  cross-references no longer work.</li>
<li>We can derive files so editors and IDEs can display in-line API docs
  as you type or can complete functions as you type, allowing people to
  code faster.</li>
</ul>
<p>While we don't generally do these things today, they are all within the
realm of possibility. Sphinx supports doing many of these things. Stop
reading and run <em>mach build-docs</em> right now and look at the warnings
from malformed documentation. I don't know about you, but I love when my
tools tell me when I'm creating a burden for others.</p>
<p>There really is so much more we could be doing with source-derived
documentation. And I argue managing it would take less overall work and
would result in higher quality documentation.</p>
<p>But the world of source-derived documentation isn't all roses. MDN has a
very important advantage: it's a wiki. Just log in, edit in a WYSIWYG,
and save. It's so easy. The moment we move to source-derived
documentation, we introduce the massive Firefox source repository, the
Firefox code review process, bugs/Bugzilla, version control overhead
(although versioning documentation is another plus for source-derived
documentation), landing changes, extra cost to Mozilla for building and
running those checkins (even if they contain docs-only changes, sadly),
and the time and cognitive burden associated with each one. That's a
lot of extra work compared to clicking a few buttons on MDN! <strong>Moving
documentation editing out of MDN and into the Firefox patch submission
world would be a step in the wrong direction in terms of fostering
contributions.</strong> Should someone really have to go through all that just
to correct a typo? I have no doubt we'd lose contributors if we switched
the change contribution process. And considering our lackluster track
record of writing inline documentation in source, I don't feel great
about losing <em>any</em> person who contributes documentation, no matter how
small the contribution.</p>
<p>And this is my dilemma: the existing source-or-MDN solution is sub-par
for pretty much everything except ease of contribution on MDN and
deploying nice tools (like Sphinx) to address the suckitude will result
in more difficulty contributing. Both alternatives suck.</p>
<p>I intend to continue this train of thought in a subsequent post. Stay
tuned.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Why hg.mozilla.org is Slow]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/12/19/why-hg.mozilla.org-is-slow" />
    <id>http://gregoryszorc.com/blog/2014/12/19/why-hg.mozilla.org-is-slow</id>
    <updated>2014-12-19T14:40:00Z</updated>
    <published>2014-12-19T14:40:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Why hg.mozilla.org is Slow]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/12/19/why-hg.mozilla.org-is-slow"><![CDATA[<p>At Mozilla, I often hear statements like <em>Mercurial is slow.</em> That's a
very general statement. Depending on the context, it can mean one or
more of several things:</p>
<ul>
<li>My Mercurial workflow is not very efficient</li>
<li><em>hg</em> commands I execute are slow to run</li>
<li><em>hg</em> commands I execute appear to stall</li>
<li>The Mercurial server I'm interfacing with is slow</li>
</ul>
<p>I want to spend time talking about a specific problem: why
hg.mozilla.org (the server) is slow.</p>
<h2>What Isn't Slow</h2>
<p><strong>If you are talking to hg.mozilla.org over HTTP or HTTPS
(https://hg.mozilla.org/), there should not currently be any server
performance issues</strong>. Our Mercurial HTTP servers are pretty beefy and
are able to absorb a lot of load.</p>
<p>If https://hg.mozilla.org/ is slow, chances are:</p>
<ul>
<li>You are doing something like cloning a 1+ GB repository.</li>
<li>You are asking the server to do something really expensive (like
  generate JSON for 100,000 changesets via the pushlog query interface).</li>
<li>You don't have a high bandwidth connection to the server.</li>
<li>There is a network event.</li>
</ul>
<h2>Previous Network Events</h2>
<p>There have historically been network capacity issues in the datacenter
where hg.mozilla.org is hosted (SCL3).</p>
<p>During Mozlandia, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1107156">excessive traffic to ftp.mozilla.org</a>
essentially saturated the SCL3 network. During this time, requests to
hg.mozilla.org were timing out: Mercurial traffic just couldn't traverse
the network. Fortunately, events like this are quite rare.</p>
<p>Up until recently, Firefox release automation was effectively
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1096337">overwhelming the network</a>
by doing some clownshoesy things.</p>
<p>For example, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1096653">gaia-central was being cloned all the time</a>
We had a ~1.6 GB repository being cloned over a thousand times per day. We
were transferring close to 2 TB of gaia-central data out of Mercurial servers
per day</p>
<p>We also found issues with
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1096650">pushlogs sending 100+ MB responses</a>.</p>
<p>And the <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1100574">build/tools repo was getting cloned for every job</a>. Ditto for mozharness.</p>
<p>In all, <strong>we identified a few terabytes of excessive Mercurial traffic
that didn't need to exist</strong>. This excessive traffic was saturating the
SCL3 network and slowing down not only Mercurial traffic, but other
traffic in SCL3 as well.</p>
<p>Fortunately, people from Release Engineering were quick to respond to
and fix the problems once they were identified. The problem is now
firmly in control. Although, given the scale of Firefox's release
automation, any new system that comes online that talks to version
control is susceptible to causing server outages. I've already raised
this concern when reviewing some TaskCluster code. The thundering herd
of automation will be an ongoing concern. But I have plans to further
mitigate risk in 2015. Stay tuned.</p>
<p>Looking back at our historical data, it appears that we hit these
network saturation limits a few times before we reached a tipping point
in early November 2014. Unfortunately, we didn't realize this because up
until recently, we didn't have a good source of data coming from the
servers. We lacked the tooling to analyze what we had. We lacked the
experience to know what to look for. Outages are effective flashlights.
We learned a lot and know what we need to do with the data moving
forward.</p>
<h2>Available Network Bandwidth</h2>
<p>One person pinged me on IRC with the comment <em>Git is cloning much faster
than Mercurial.</em> I asked for timings and the Mercurial clone wall time
for Firefox was much higher than I expected.</p>
<p>The reason was network bandwidth. This person was performing a Git clone
between 2 hosts in EC2 but was performing the Mercurial clone between
hg.mozilla.org and a host in EC2. In other words, they were partially
comparing the performance of a 1 Gbps network against a link over the
public internet! When they did a fair comparison by removing the network
connection as a variable, the clone times rebounded to what I expected.</p>
<p>The single-homed nature of hg.mozilla.org in a single datacenter in
northern California is not only bad for disaster recovery reasons, it
also means that machines far away from SCL3 or connecting to SCL3 over a
slow network aren't getting optimal performance.</p>
<p>In 2015, expect us to build out a geo-distributed hg.mozilla.org so that
connections are hitting a server that is closer and thus faster. This
will probably be targeted at Firefox release automation in AWS first. We
want those machines to have a fast connection to the server <strong>and</strong> we
want their traffic isolated from the servers developers use so that
hiccups in automation don't impact the ability for humans to access
and interface with source code.</p>
<h2>NFS on SSH Master Server</h2>
<p>If you connect to http://hg.mozilla.org/ or https://hg.mozilla.org/, you
are hitting a pool of servers behind a load balancer. These servers have
repository data stored on local disk, where I/O is fast. In reality,
most I/O is serviced by the page cache, so local disks don't come into
play.</p>
<p>If you connect to ssh://hg.mozilla.org/, you are hitting a single,
master server. Its repository data is hosted on an NFS mount. I/O on the
NFS mount is horribly slow. Any I/O intensive operation performed on the
master is much, much slower than it should be. Such is the nature of
NFS.</p>
<p>We'll be exploring ways to mitigate this performance issue in 2015. But
it isn't the biggest source of performance pain, so don't expect anything
immediately.</p>
<h2>Synchronous Replication During Pushes</h2>
<p>When you <em>hg push</em> to hg.mozilla.org, the changes are first made on the
SSH/NFS master server. They are subsequently mirrored out to the HTTP
read-only slaves.</p>
<p>As is currently implemented, the mirroring process is performed
synchronously during the push operation. The server waits for the
mirrors to complete (to a reasonable state) before it tells the client
the push has completed.</p>
<p>Depending on the repository, the size of the push, and server and
network load, <strong>mirroring commonly adds 1 to 7 seconds to push times</strong>.
This is time when a developer is sitting at a terminal, waiting for <em>hg
push</em> to complete. The time for Try pushes can be larger: 10 to 20
seconds is not uncommon (but fortunately not the norm).</p>
<p>The current mirroring mechanism is overly simple and prone to many
failures and sub-optimal behavior. I plan to work on fixing mirroring in
2015. When I'm done, there should be no user-visible mirroring delay.</p>
<h2>Pushlog Replication Inefficiency</h2>
<p>Up until yesterday (when we
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1112415">deployed a rewritten pushlog extension</a>,
the replication of pushlog data from master to server was very
inefficient. Instead of tranferring a delta of pushes since last pull,
we were literally copying the underlying SQLite file across the network!</p>
<p>Try's pushlog is ~30 MB. mozilla-central and mozilla-inbound are in the
same ballpark. 30 MB x 10 slaves is a lot of data to transfer. These
operations were capable of periodically saturating the network, slowing
everyone down.</p>
<p>The rewritten pushlog extension performs a delta transfer automatically
as part of <em>hg pull</em>. Pushlog synchronization now completes in
milliseconds while commonly only consuming a few kilobytes of network
traffic.</p>
<p>Early indications reveal that deploying this change yesterday decreased the
push times to repositories with long push history by 1-3s.</p>
<h2>Try</h2>
<p>Pretty much any interaction with the
<a href="https://hg.mozilla.org/try">Try repository</a> is guaranteed to have poor
performance. The Try repository is doing things that distributed
versions control systems weren't designed to do. This includes Git.</p>
<p>If you are using Try, all bets are off. Performance will be problematic
until we roll out the <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1055298">headless try</a>
repository.</p>
<p>That being said, we've made changes recently to make Try perform better.
<a href="https://twitter.com/lxt/status/546042588822642688">The median time for pushing to Try</a>
has decreased significantly in the past few weeks. The first dip in
mid-November was due to upgrading the server from Mercurial 2.5 to
Mercurial 3.1 and from converting Try to use <em>generaldelta</em> encoding.
The dip this week has been from merging all heads and from deploying the
aforementioned pushlog changes. <strong>Pushing to Try is now significantly
faster than 3 months ago.</strong></p>
<h2>Conclusion</h2>
<p>Many of the reasons for hg.mozilla.org slowness are known. More often
than not, they are due to clownshoes or inefficiencies on Mozilla's
part rather than fundamental issues with Mercurial.</p>
<p><strong>We have made significant progress at making hg.mozilla.org faster.</strong>
But we are not done. We are continuing to invest in fixing the
sub-optimal parts and making hg.mozilla.org faster yet. I'm confident
that within a few months, nobody will be able to say that the servers
are a source of pain like they have been for years.</p>
<p>Furthermore, Mercurial is investing in features to make the wire
protocol faster, more efficient, and more powerful. When deployed,
these should make pushes faster on <em>any</em> server. They will also enable
workflow enhancements, such as Facebook's experimental extension to
perform rebases as part of push (eliminating push races and having to
manually rebase when you lose the push race).</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[mach sub-commands]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/12/18/mach-sub-commands" />
    <id>http://gregoryszorc.com/blog/2014/12/18/mach-sub-commands</id>
    <updated>2014-12-18T09:45:00Z</updated>
    <published>2014-12-18T09:45:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[mach sub-commands]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/12/18/mach-sub-commands"><![CDATA[<p><a href="https://pypi.python.org/pypi/mach/">mach</a> - the generic command line
dispatching tool that powers the <strong>mach</strong> command to aid Firefox
development - now has support for sub-commands.</p>
<p>You can now create simple and intuitive user interfaces involving
sub-actions. e.g.</p>
<pre><code>mach device sync
mach device run
mach device delete
</code></pre>
<p>Before, to do something like this would require a universal argument
parser or separate mach commands. Both constitute a poor user
experience (confusing array of available arguments or proliferation of
top-level commands). Both result in <em>mach help</em> being difficult to
comprehend. And that's not good for usability and approachability.</p>
<p>Nothing in Firefox currently uses this feature. Although there is an
in-progress patch in
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1108293">bug 1108293</a>
for providing a mach command to analyze C/C++ build dependencies.
It is my hope that others write useful commands and functionality
on top of this feature.</p>
<p>The documentation for mach has also been rewritten. It is now
<a href="https://ci.mozilla.org/job/mozilla-central-docs/Tree_Documentation/python/mach/index.html">exposed</a>
as part of the in-tree Sphinx documentation.</p>
<p>Everyone should thank <a href="https://mozillians.org/en-US/u/ahal/">Andrew Halberstadt</a>
for promptly reviewing the changes!</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[A Crazy Day]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/12/04/a-crazy-day" />
    <id>http://gregoryszorc.com/blog/2014/12/04/a-crazy-day</id>
    <updated>2014-12-04T23:34:00Z</updated>
    <published>2014-12-04T23:34:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[A Crazy Day]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/12/04/a-crazy-day"><![CDATA[<p>Today was one crazy day.</p>
<p>The build peers all sat down with Release Engineering and Axel Hecht
to talk l10n packaging. <a href="http://glandium.org/">Mike Hommey</a> has a
Q1 goal to fix l10n packaging. There is buy-in from Release
Engineering on enabling him in his quest to slay dragons. This
will make builds faster and will pay off a massive mountain of
technical debt that plagues multiple groups at Mozilla.</p>
<p>The Firefox build system contributors sat down with a bunch of
Rust people and we talked about what it would take to integrate
Rust into the Firefox build system and the path towards shipping
a Rust component in Firefox. It sounds like that is going to
happen in 2015 (although we're not yet sure what component will
be written in Rust). I consider it an achievement that the gathering
of both groups didn't result in infinite rabbit holing about
system architectures, toolchains, and the build people telling
<em>horror stories</em> to wide-eyed Rust people about the crazy things
we have to do to build and ship Firefox. Believe me, the
temptation was there.</p>
<p>People interested in the build system all sat down and reflected on
the state of the build system and where we want to go. We agreed
to create a build mode optimized for non-Gecko developers
that downloads pre-built binaries - avoiding ~10 minutes of
C/C++ compile time for builds. Mark my words, this will be one
of those changes that once deployed will cause people to say
"I can't believe we went so long without this."</p>
<p>I joined Mark Côté and others to talk about priorities for
MozReview. We'll be making major improvements to the UX and
integrating static analysis into reviews. Push a patch for
review and have machines do some of the work that humans are
currently doing! We're tentatively planning a get-together in
Toronto in January to sprint towards awesomeness.</p>
<p>I ended the day by giving a long and rambling presentation about
version control, with emphasis on Mercurial. I can't help but
feel that I talked way too much. There's just so much knowledge
to cover. A few people told me afterwards they learned a lot. I'd
appreciate feedback if you attended. Anyway, I got a few nods
from people as I was saying some somewhat contentious things.
It's always good to have validation that I'm not on an island
when I say crazy things.</p>
<p>I hope to spend Friday chasing down loose ends from the week.
This includes talking to some security gurus about another crazy
idea of mine to integrate PGP into the code review and code
landing workflow for Firefox. I'll share more details once
I get a professional opinion on the security front.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[The Mozlandia Tree Outage and Code Review]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/12/04/the-mozlandia-tree-outage-and-code-review" />
    <id>http://gregoryszorc.com/blog/2014/12/04/the-mozlandia-tree-outage-and-code-review</id>
    <updated>2014-12-04T08:40:00Z</updated>
    <published>2014-12-04T08:40:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Review Board" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <category scheme="http://gregoryszorc.com/blog" term="code review" />
    <summary type="html"><![CDATA[The Mozlandia Tree Outage and Code Review]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/12/04/the-mozlandia-tree-outage-and-code-review"><![CDATA[<p>You may have noticed the Firefox trees
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1107156">were closed</a>
for the better part of yesterday.</p>
<p>Long story short, a file containing URLs for Firefox installers was
updated to reference https://ftp.mozilla.org/ from
http://download-installer.cdn.mozilla.net/. The original, latter
host is a CDN. The former is not. When thousands of clients started
hitting ftp.mozilla.org, it overwhelmed the servers and network,
causing timeouts and other badness.</p>
<p>The change in question was accidental. It went through code review.
From a code change standpoint, procedures were followed.</p>
<p>It is tempting to point fingers at the people involved. However, I
want us to consider placing blame elsewhere: on the code review
tool.</p>
<p>The diff being reviewed was to change the Firefox version number
from 32.0 to 32.0.3. If you were asked to review this
patch, I'm guessing your eyes would have glanced over everything in the
URL except the version number part. I'm pretty sure mine would have.</p>
<p>Let's take a look at what the reviewer saw in Bugzilla/Splinter (click
to see full size):</p>
<p><a href="/images/intraline-splinter.png">
<img src="/images/intraline-splinter.png" width="600" />
</a></p>
<p>And here is what the reviewer would have seen had the review been
conducted in MozReview:</p>
<p><a href="/images/intraline-reviewboard.png">
<img src="/images/intraline-reviewboard.png" width="600">
</a></p>
<p>Which tool makes the change of hostname more obvious? Bugzilla/Splinter
or MozReview?</p>
<p><strong>MozReview's support for intraline diffs more clearly draws attention to
the hostname change. I posit that had this patch been reviewed with
MozReview, the chances are greater we wouldn't have had a network
outage yesterday.</strong></p>
<p>And it isn't just intraline diffs that make Splinter/Bugzilla a
sub-optimal code review tool. I recently blogged about the
<a href="/blog/2014/10/27/implications-of-using-bugzilla-for-firefox-patch-development/">numerous ways that using Bugzilla for code revie results in harder
reviews and buggier
code</a>.
Every day we continue using Bugzilla/Splinter instead of investing in
better code review tools is a day severe bugs like this can and will
slip through the cracks.</p>
<p>If there is any silver lining to this outage, I hope it is that we need
to double down on our investment in developer tools, particularly code
review.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Test Drive the New Headless Try Repository]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/11/20/test-drive-the-new-headless-try-repository" />
    <id>http://gregoryszorc.com/blog/2014/11/20/test-drive-the-new-headless-try-repository</id>
    <updated>2014-11-21T18:50:00Z</updated>
    <published>2014-11-20T14:45:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Test Drive the New Headless Try Repository]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/11/20/test-drive-the-new-headless-try-repository"><![CDATA[<p>Mercurial and Git both experience scaling pains as the number of heads
in a repository approaches infinity. Operations like push and pull
slow to a crawl and everyone gets frustrated.</p>
<p>This is the problem Mozilla's Try repository has been dealing with
for years. We know the solution doesn't scale. But we've been content
kicking the can by resetting the repository (blowing away data)
to make the symptoms temporarily go away.</p>
<p>One of my official goals is to ship a scalable Try solution by the end
of 2014.</p>
<p>Today, I believe I finally have enough code cobbled together to
produce a working concept. <strong>And I could use your help testing it.</strong></p>
<p>I would like people to push their Try, code review, and other
miscellaneous heads to a special repository. To do this:</p>
<pre><code>$ hg push -r . -f ssh://hg@hg.gregoryszorc.com/gecko-headless
</code></pre>
<p>That is:</p>
<ul>
<li>Consider the changeset belonging to the working copy</li>
<li>Allow the creation of new heads</li>
<li>Send it to the <em>gecko-headless</em> repo on hg.gregoryszorc.com using
  SSH</li>
</ul>
<p>Here's what happening.</p>
<p>I have deployed a special repository to my personal server that I
believe will behave very similarly to the final solution.</p>
<p>When you push to this repository, instead of your changesets being
applied directly to the repository, it siphons them off to a Mercurial
bundle. It then saves this bundle somewhere along with some metadata
describing what is inside.</p>
<p>When you run <em>hg pull -r <node></em> on that repository and ask for a
changeset that exists in the bundle, the server does some magic
and returns data from the bundle file.</p>
<p>Things this repository doesn't do:</p>
<ul>
<li><strong>This repository will not actually send changesets to Try for you.</strong></li>
<li>You cannot <code>hg pull</code> or <code>hg clone</code> the repository and get all of the
  commits from bundles. This isn't a goal. It will likely never be
  supported.</li>
<li>We do not yet record a pushlog entry for pushes to the repository.</li>
<li>The hgweb HTML interface does not <strong>yet</strong> handle commits that only
  exist in bundles. People want this to work. It will eventually work.</li>
<li>Pulling from the repository over HTTP with a vanilla Mercurial install
  may not preserve phase data.</li>
</ul>
<p>The purpose of this experiment is to expose the repository to some
actual traffic patterns so I can see what's going on and get a feel
for real-world performance, variability, bugs, etc. I plan to do all
of this in the testing environment. But I'd like some real-world
use on the actual Firefox repository to give me peace of mind.</p>
<p>Please report any issues directly to me. Leave a comment here. Ping
me on IRC. Send me an email. etc.</p>
<p><strong>Update 2014-11-21: People discovered a bug with pushed changesets
accidentally being advanced to the public phase, despite the repository
being non-publishing. I have fixed the issue. But you must now push to
the repository over SSH, not HTTP.</strong></p>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Mercurial Server Hiccup 2014-11-06]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/11/07/mercurial-server-hiccup-2014-11-06" />
    <id>http://gregoryszorc.com/blog/2014/11/07/mercurial-server-hiccup-2014-11-06</id>
    <updated>2014-11-07T11:00:00Z</updated>
    <published>2014-11-07T11:00:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Mercurial Server Hiccup 2014-11-06]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/11/07/mercurial-server-hiccup-2014-11-06"><![CDATA[<p>We had a hiccup on hg.mozilla.org yesterday. It resulted in prolonged
tree closures for Firefox.
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1094922">Bug 1094922</a>
tracks the issue.</p>
<h2>What Happened</h2>
<p>We noticed that many HTTP requests to hg.mozilla.org were getting 503
responses. On initial glance, the servers were healthy. CPU was below
100% utilization, I/O wait was reasonable. And there was little to no
swapping. Furthermore, the logs showed a healthy stream of requests
being successfully processed at levels that are typical. In other words,
it looked like business as usual on the servers.</p>
<p>Upon deeper investigation, we noticed that the WSGI process pool on the
servers was fully saturated. There were were 24 slots/processes per server
allocated to process Mercurial requests and all 24 of them were actively
processing requests. This created a backlog of requests that had been
accepted by the HTTP server but were waiting an internal dispatch/proxy
to WSGI. To the client, this would appear as a request with a long lag
before response generation.</p>
<h2>Mitigation</h2>
<p>This being an emergency (trees were already closed and developers were
effectively unable to use hg.mozilla.org), we decided to increase the
size of the WSGI worker pool. After all, we had CPU, I/O, and memory
capacity to spare and we could identify the root cause later. We first
bumped worker pool capacity from 24 to 36 and immediately saw a significant
reduction in the number of pending requests awaiting a WSGI worker. We
still had spare CPU, I/O, and memory capacity and were still seeing
requests waiting on a WSGI worker, so we bumped the capacity to 48
processes. At that time, we stopped seeing worker pool exhaustion and
all incoming requests were being handed off to a WSGI worker as soon as
they came in.</p>
<p>At this time, things were looking pretty healthy from the server end.</p>
<h2>Impact on Memory and Swapping</h2>
<p>Increasing the number of WSGI processes had the side-effect of
increasing the total amount of system memory used by Mercurial
processes in two ways. First, more processes means more memory. That
part is obvious. Second, more processes means fewer requests for each
process per unit of time and thus it takes longer for each process to
reach its max number of requests being being reaped. (It is a common
practice in servers to have a single process hand multiple requests.
This prevents overhead associated with spawning a new process and
loading possibly expensive context in it.)</p>
<p>We had our Mercurial WSGI processes configured to serve 100 requests
before being reaped. With the doubling of WSGI processes from 24 to 48,
those processes were lingering for 2x as long as before. Since the
Mercurial processes grow over time (they are aggressive about caching
repository data), this was slowly exhausting our memory pool.</p>
<p>It took a few hours, but a few servers started flirting with high swap
usage. (We don't expect the servers to swap.) This is how we identified
that memory use wasn't sane.</p>
<p>We lowered the maximum requests per process from 100 to 50 to match
the ratio increase in the WSGI worker pool.</p>
<h2>Mercurial Memory "Leak"</h2>
<p>When we started looking at the memory usage of WSGI processes in more
detail, we noticed something strange: RSS of Mercurial processes was
growing steadily when processes were streaming bundle data. This seemed
very odd to me. Being a Mercurial developer, I was pretty sure the
behavior was wrong.</p>
<p>I <a href="http://bz.selenic.com/show_bug.cgi?id=4443">filed a bug</a> against
Mercurial.</p>
<p>I was able to reproduce the issue locally and started running a
bisection to find the regressing changeset. To my surprise, this issue
has been around since Mercurial 2.7!</p>
<p>I looked at the code in question, identified why so much memory was
being allocated, and submitted patches to
<a href="http://www.selenic.com/pipermail/mercurial-devel/2014-November/063518.html">stop an unbounded memory growth during clone/pull</a>
and to
<a href="http://www.selenic.com/pipermail/mercurial-devel/2014-November/063520.html">further reduce memory use during those operations</a>.
Both of those patches have been queued to go in the next stable release
of Mercurial, 3.2.1.</p>
<p>Mercurial 3.2 is still not as memory efficient during clones as Mercurial
2.5.4. If I have time, I'd like to formulate more patches. But the
important fix - not growing memory unbounded during clone/pull - is
in place.</p>
<p>Armed with the knowledge that Mercurial is leaky (although not a leak
in the traditional sense since the memory was eventually
getting garbage collected), we further reduced the max requests per
process from 50 to 20. This will cause processes to get reaped sooner
and will be more aggressive about keeping RSS growth in check.</p>
<h2>Root Cause</h2>
<p>We suspect the root cause of the event is a network event.</p>
<p>Before this outage, we rarely had more than 10 requests being served
from the WSGI worker pool. In other words, we were often well below 50%
capacity. But something changed yesterday. More slots were being
occupied and high-bandwidth operations were taking longer to complete.
Kendall Libby noted that outbound traffic dropped by ~800 Mbps
during the event. For reasons that still haven't been identified, the
network became slower, clones weren't being processed as quickly, and
clients were occupying WSGI processes for longer amounts of time. This
eventually exhausted the available process pool, leading to HTTP 503's,
intermittent service availability, and a tree closure.</p>
<p>Interestingly, we noticed that in-flight HTTP requests are abnormally
high again this morning. However, because the servers are now configured
to handle the extra capacity, we seem to be powering through it without
any issues.</p>
<h2>In Hindsight</h2>
<p>You can make the argument that the servers weren't configured to serve
as much traffic as possible. After all, we were able to double the
WSGI process pool without hitting CPU, I/O, and memory limits.</p>
<p>The servers were conservatively configured. However, the worker pool
was initially configured at 2x CPU core count. And as a general rule of
thumb, you don't want your worker pool to be much greater than CPU
count because that introduces context switching and can give each
individual process a smaller slice of the CPU to process requests,
leading to higher latency. Since clone operations often manage to
peg a single CPU core, there is some justification for keeping the
ratio of WSGI workers to CPU count low. Furthermore, we rarely came
close to exhausting the WSGI worker pool before. There was little
to no justification for increasing capacity to a threshold not normally
seen.</p>
<p>But at the same time, even with 4x workers to CPU cores, our CPU
usage rarely flirts with 100% across all cores, even with the
majority of workers occupied. Until we actually hit CPU (or I/O)
limits, running a high multiplier seems like the right thing to do.</p>
<p>Long term, we expect CPU usage during clone operations to drop
dramatically. Mike Hommey has contributed a patch to Mercurial that
allows servers to hand out a URL of a bundle file to fetch during
clone. So, a server can say <em>I have your data: fetch this static file
from S3 and then apply this small subset of the data that I'll give
you.</em> When properly deployed and used at Mozilla, this will effectively
drop server-side CPU usage for clones to nothing.</p>
<h2>Where to do Better</h2>
<p>There was a long delay between the Nagios alerts firing and someone
with domain-specific knowledge looking at the problem.</p>
<p>The trees could have reopened earlier. We were pretty confident about
the state of things at 1000. Trees opened in metered mode at 1246 and
completely at 1909. Although, the swapping issue wasn't mitigated until
1615, so you can argue that being conservative on the tree reopening
was justified. There is a chance that full reopening could have
triggered excessive swap and another round of chaos for everyone
involved.</p>
<p>We need an alert on WSGI pool exhaustion. It took longer than it should
have to identify this problem. However, now that we've encountered it,
it should be obvious if/when it happens again.</p>
<p>Firefox release automation is the largest single consumer of
hg.mozilla.org. Since they are operating thousands of machines, any
reduction in interaction or increase in efficiency will result in
drastic load reductions on the server. Chris AtLee and Jordan Lund
have been working on
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1050109">bug 1050109</a> to
reduce clones of the mozharness and build/tools repositories, which
should go a long way to dropping load on the server.</p>
<h2>Timeline of Events</h2>
<p>All times PST.</p>
<p>November 6</p>
<ul>
<li>0705 - First Nagios alerts fire</li>
<li>0819 - Trees closed</li>
<li>0915 - WSGI process pool increased from 24 to 36</li>
<li>0945 - WSGI process pool increased from 36 to 48</li>
<li>1246 - Trees reopen in metered mode</li>
<li>1615 - Decrease max requests per process from 100 to 50</li>
<li>1909 - Trees open completely</li>
</ul>
<p>November 7</p>
<ul>
<li>0012 - Patches to reduce memory usage submitted to Mercurial</li>
<li>0800 - Mercurial patches accepted</li>
<li>0915 - Decrease max requests per process from 50 to 20</li>
</ul>]]></content>
  </entry>
  <entry>
    <author>
      <name>Gregory Szorc</name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Soft Launch of MozReview]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/10/30/soft-launch-of-mozreview" />
    <id>http://gregoryszorc.com/blog/2014/10/30/soft-launch-of-mozreview</id>
    <updated>2014-10-30T11:15:00Z</updated>
    <published>2014-10-30T11:15:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Review Board" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <category scheme="http://gregoryszorc.com/blog" term="code review" />
    <summary type="html"><![CDATA[Soft Launch of MozReview]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/10/30/soft-launch-of-mozreview"><![CDATA[<p>We performed a soft launch of MozReview: Mozilla's new code review tool
yesterday!</p>
<p>What does that mean? How do I use it? What are the features?
How do I get in touch or contribute? These are all great questions.
The answers to those and more can all be found in the
<a href="https://mozilla-version-control-tools.readthedocs.org/en/latest/mozreview.html">MozReview documentation</a>.
If they aren't, it's a bug in the documentation. File a bug or submit a
patch. Instructions to do that are in the documentation.</p>]]></content>
  </entry>
</feed>
