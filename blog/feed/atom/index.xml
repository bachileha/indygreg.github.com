<?xml version="1.0" encoding="UTF-8"?>
<feed
  xmlns="http://www.w3.org/2005/Atom"
  xmlns:thr="http://purl.org/syndication/thread/1.0"
  xml:lang="en"
   >
  <title type="text">Gregory Szorc's Digital Home</title>
  <subtitle type="text">Rambling on</subtitle>

  <updated>2014-04-01T19:31:27Z</updated>
  <generator uri="http://blogofile.com/">Blogofile</generator>

  <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog" />
  <id>http://gregoryszorc.com/blog/feed/atom/</id>
  <link rel="self" type="application/atom+xml" href="http://gregoryszorc.com/blog/feed/atom/" />
  <entry>
    <author>
      <name></name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Using Mercurial for Status Reports]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/04/01/using-mercurial-for-status-reports" />
    <id>http://gregoryszorc.com/blog/2014/04/01/using-mercurial-for-status-reports</id>
    <updated>2014-04-01T12:30:00Z</updated>
    <published>2014-04-01T12:30:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Using Mercurial for Status Reports]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/04/01/using-mercurial-for-status-reports"><![CDATA[<p>Mercurial has a pair of amazing features called
<a href="http://www.selenic.com/hg/help/revsets">Revisions Sets</a>
and <a href="http://www.selenic.com/hg/help/templates">Templates</a>. Combined,
they allow you to query Mercurial like a database and to generate custom
reports from obtained data.</p>
<p>As I've <a href="/blog/2013/11/08/using-mercurial-to-query-mozilla-metadata/">demonstrated</a>,
you can write Mercurial extensions to provide custom revision set
queries and template functions and keywords. My
<a href="https://hg.mozilla.org/hgcustom/version-control-tools/file/default/hgext/mozext">mozext</a>
extension aggregates Mozilla's <em>pushlog</em> data into a local SQLite
database and makes this data available to revision sets and templates.</p>
<p>My hack of the day is to use revision sets and templates to create a
weekly status report:</p>
<div class="pygments_murphy"><pre>hg log -r &#39;public() and me() and firstpushdate(&quot;-7&quot;)&#39; \
--template &#39;* {ifeq(reviewer, &quot;gps&quot;, &quot;Review: &quot;, &quot;Landing: &quot;)}{firstline(desc)}\n&#39;
</pre></div>

<p>When I run this, I get the output:</p>
<div class="pygments_murphy"><pre>* Review: Bug 957241 - Don&#39;t package the full sdk when we don&#39;t need it. r=gps
* Review: Bug 987146 - Represent SQL queries more efficiently. r=gps.
* Review: Bug 987984 - VirtualenvManager.call_setup() should use self.python_path instead of sys.executable, r=gps
* Landing: Bug 987398 - Part 1: Run mochitests from manifests with mach; r=ahal
* Landing: Bug 987398 - Part 2: Handle install-to-subdir in TestResolver; r=ahal
* Landing: Bug 987414 - Pass multiple test arguments to mach testing commands; r=ahal
* Review: Bug 988141 - Clean up config/recurse.mk after bug 969164. r=gps
* Landing: Bug 973992 - Support experiments add-ons; r=Unfocused
* Review: Bug 927672 - Force pymake to fall back to mozmake when run on build slaves. r=gps
* Review: Bug 989147 - Use new sccache for Linux and Android builds. r=gps
* Review: Bug 989147 - Add missing part of the patch from rebase conflict. r=gps
* Landing: Bug 975000 - Disable updating and compatibility checking for Experiments; r=Unfocused
* Landing: Bug 985084 - Experiment add-ons should be disabled by default; r=Unfocused
* Landing: Backed out changeset 4834a3833639 and c580afddd1cb (bug 985084 and bug 97500)
* Landing: Bug 975000 - Disable updating and compatibility checking for Experiments; r=Unfocused
* Landing: Bug 985084 - Experiment add-ons should be disabled by default; r=Unfocused
* Landing: Bug 989137 - Part 1: Uninstall unknown experiments; r=Unfocused
* Landing: Bug 989137 - Part 2: Don&#39;t use a global logger; r=gfritzsche
* Landing: Bug 989137 - Part 3: Log.jsm API to get a Logger that prefixes messages; r=bsmedberg
* Landing: Bug 989137 - Part 4: Use a prefixing logger for Experiments logging; r=gfritzsche
* Landing: Bug 989137 - Part 5: Prefix each log message with the instance of the object; r=gfritzsche
* Review: Bug 988849 - Add mach target for jit tests; r=gps
* Landing: Bug 989137 - Part 6: Create experiment XPIs during the build; r=bsmedberg
* Landing: Bug 989137 - Part 7: Remove unncessary content from test experiments; r=Unfocused
* Landing: Bug 985084 - Part 2: Properly report userDisabled in the API; r=Unfocused
</pre></div>

<p>Which I can then copy and paste directly into the
<a href="http://benjamin.smedbergs.us/weekly-updates.fcgi/">status tool</a> to
capture all my weekly code contributions! That takes a few seconds to
run and saves me a few minutes of typing.</p>
<p>For the curious, let's break that Mercurial command down.</p>
<ul>
<li>public() selects all <em>public</em> changesets. These are changesets in the
  repository that have been pushed to a publishing repository. In other
  words, patches that landed in Firefox.</li>
<li>me() is a custom revset from my <em>mozext</em> extension that parses the
  commit message and selects changesets that I authored or reviewed.</li>
<li>firstpushdate("-7") is a custom revset from my <em>mozext</em> extension. It
  selects changesets that were first pushed in the last 7 days (using
  pushlog data stored in a local SQLite database).</li>
</ul>
<p>The template piece should be easy to read. I have a simple branch
testing whether the changeset is a review or not, then output a label
followed by the first line of the commit message.</p>
<p>I have this command saved under the <em>[alias]</em> section of my <em>~/.hgrc</em>
file so I can just type <em>hg statusreport</em>.</p>
<p>While there is room to improve the tool (stripping <em>r=</em> lines from
commit messages for example), I think it's a pretty cool hack and shows
how Mercurial can grow to solve problems you don't think your version
control system knows how to solve.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[How Promises and Tasks are Improving Tests]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/03/30/how-promises-and-tasks-are-improving-tests" />
    <id>http://gregoryszorc.com/blog/2014/03/30/how-promises-and-tasks-are-improving-tests</id>
    <updated>2014-03-30T14:15:00Z</updated>
    <published>2014-03-30T14:15:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <category scheme="http://gregoryszorc.com/blog" term="JavaScript" />
    <summary type="html"><![CDATA[How Promises and Tasks are Improving Tests]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/03/30/how-promises-and-tasks-are-improving-tests"><![CDATA[<p>I was a very early adoptor of promises and
<a href="https://developer.mozilla.org/en-US/docs/Mozilla/JavaScript_code_modules/Task.jsm">Tasks</a>
in Firefox's JavaScript code base. To me, promises on their own are ok.
The ability to chain promises together and tack one error handler on
the end sure beats the
<a href="http://tritarget.org/blog/2012/11/28/the-pyramid-of-doom-a-javascript-style-trap/">Pyramid of Doom</a>
and having to pass errors into callbacks everywhere. But what really
lured me in were tasks: using generators (then a feature only available
in SpiderMonkey) to represent async code flow as nice, easy-to-read
procedural flow that nearly every programming can relate to. It made
code much easier to read and grok. I've been using tasks ever since.</p>
<p>When I started writing new APIs that returned promises instead of using
callbacks, I found myself writing a lot of tests consuming promises and
using tasks. So, I
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=819033">added an add_task</a>
API to our xpcshell test harness to make writing task-based unit tests
involve less boilerplate. That API is now used heavily for new xpcshell
tests.</p>
<p>While I initially added <em>add_task()</em> to cut down on the boilerplate for
writing tests, I only recently realized it has another benefit: it's
helped cut down on hung tests!</p>
<p>Before, with callback-based APIs, we'd code tests like so:</p>
<div class="pygments_murphy"><pre><span class="nx">add_test</span><span class="p">(</span><span class="kd">function</span> <span class="p">()</span> <span class="p">{</span>
  <span class="nx">do_something</span><span class="p">(</span><span class="kd">function</span> <span class="nx">onThatThing</span><span class="p">(</span><span class="nx">result</span><span class="p">)</span> <span class="p">{</span>
     <span class="nx">Assert</span><span class="p">.</span><span class="nx">ok</span><span class="p">(</span><span class="nx">result</span><span class="p">.</span><span class="nx">success</span><span class="p">);</span>
     <span class="nx">run_next_test</span><span class="p">();</span>
  <span class="p">});</span>
<span class="p">});</span>
</pre></div>

<p>Or another pattern:</p>
<div class="pygments_murphy"><pre><span class="nx">add_test</span><span class="p">(</span><span class="kd">function</span> <span class="p">()</span> <span class="p">{</span>
  <span class="nx">do_something</span><span class="p">(</span><span class="kd">function</span> <span class="nx">onThatThing</span><span class="p">(</span><span class="nx">result</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// The next line throws an Error by accident!</span>
    <span class="nx">result</span><span class="p">.</span><span class="nx">foo</span><span class="p">();</span>
    <span class="nx">run_next_test</span><span class="p">();</span>
  <span class="p">});</span>
<span class="p">});</span>
</pre></div>

<p>In the first example, the test will hang if the callback never gets
called. The test harness driver will eventually terminate
the test (after a multi-second delay with no output). Not good.</p>
<p>In the second example, we are still susceptible to the callback not
being called. But we have a different problem: an untrapped Error is
thrown from a callback! This results in the same behavior:
<em>run_next_test()</em> (the function that says to advance to the next test)
won't execute and the test will hang until it times out.</p>
<p>A more proper way to write this test is:</p>
<div class="pygments_murphy"><pre><span class="nx">add_test</span><span class="p">(</span><span class="kd">function</span> <span class="p">()</span> <span class="p">{</span>
  <span class="nx">do_something</span><span class="p">(</span><span class="kd">function</span> <span class="nx">onThatThing</span><span class="p">(</span><span class="nx">result</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">try</span> <span class="p">{</span>
      <span class="nx">result</span><span class="p">.</span><span class="nx">foo</span><span class="p">();</span>
    <span class="p">}</span> <span class="k">catch</span> <span class="p">(</span><span class="nx">ex</span><span class="p">)</span> <span class="p">{</span>
      <span class="nx">do_report_unexpected_exception</span><span class="p">(</span><span class="nx">ex</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nx">run_next_test</span><span class="p">();</span>
  <span class="p">});</span>
<span class="p">});</span>
</pre></div>

<p>In reality, few people surround all their callbacks with try..catch
blocks because, well, it's a lot of typing and people don't always think
it's necessary (the test passes most of the time, doesn't it?).</p>
<p>What promises and task-based tests are doing is enabling us to write
more robust tests without all of the extra work. Here is how you would
use task-based tests:</p>
<div class="pygments_murphy"><pre><span class="nx">add_task</span><span class="p">(</span><span class="kd">function</span><span class="o">*</span> <span class="p">()</span> <span class="p">{</span>
  <span class="kd">let</span> <span class="nx">result</span> <span class="o">=</span> <span class="nx">yield</span> <span class="nx">do_something</span><span class="p">();</span>
  <span class="c1">// The next line throws an Error by accident!</span>
  <span class="nx">result</span><span class="p">.</span><span class="nx">foo</span><span class="p">();</span>
<span class="p">});</span>
</pre></div>

<p>Here, the Error thrown by the test function is thrown within the context
of an executing Task. It is caught by the Task and converted into a
rejected promise. <strong>The test harness sees that failure immediately
and no timeout occurs!</strong> This can cut down on overhead when writing
tests, especially if you are trying to debug a hang.</p>
<p>Furthermore, the test is 4 lines versus 10. Less typing means you have
more time to write additional tests or you can focus on writing other
patches.</p>
<p>Finally, the task-based test functions are easier to understand. That 4
line, procedural test is much easier to grok than its callback-based
counterpart.</p>
<p>And before I conclude, I should mention that we can do more with
promises. For example, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=976205">bug 976205</a> is making uncaught promise errors turn into test failures!
There is also an awesome patch in
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=867742">bug 867742</a> to
introduce a unified JavaScript test harness API for defining JavaScript
tests in our tree (currently the APIs for xpcshell tests and mochitests
are different, leading to cognitive dissonance and lower productivity).
<strong>If you want to be a hero to the Firefox developer community, help
finish that patch.</strong></p>
<p>Given that so much Firefox feature development time (at Mozilla) is
spent writing and debugging tests, I encourage everyone to consider
promises and tasks for his or her next feature so that you can cut
down on development time and complete projects faster.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[New Repository for Mozilla Version Control Tools]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/02/05/new-repository-for-mozilla-version-control-tools" />
    <id>http://gregoryszorc.com/blog/2014/02/05/new-repository-for-mozilla-version-control-tools</id>
    <updated>2014-02-05T19:15:00Z</updated>
    <published>2014-02-05T19:15:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Git" />
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[New Repository for Mozilla Version Control Tools]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/02/05/new-repository-for-mozilla-version-control-tools"><![CDATA[<p>Version control systems can be highly useful tools.</p>
<p>At Mozilla, we've made numerous improvements and customizations to our
version control tools. We have <a href="https://hg.mozilla.org/hgcustom/hghooks/">custom hooks</a>
that run on the server. We have a <a href="https://hg.mozilla.org/hgcustom/hg_templates/">custom skin</a>
for Mercurial's web interface. Mozillians have written a handful of
Mercurial extensions to aid with common developer tasks, such as
<a href="https://bitbucket.org/sfink/trychooser">pushing to try</a>,
<a href="https://hg.mozilla.org/users/tmielczarek_mozilla.com/bzexport">interacting with Bugzilla</a>,
<a href="https://bitbucket.org/sfink/mqext">making mq more useful</a>, and more.</p>
<p>These have all come into existence in an organic manner, one after the
other. Individuals have seen an itch and scratched it. Good for them.
Good for Mozilla.</p>
<p>Unfortunately, the collection of amassed tools has become quite large.
They have become difficult to discover and keep up to date. The
consistency in quality and style between the tools varies. Each tool has
separate processes for updating and changing.</p>
<p>I contacted the maintainers of the popular version control tools at
Mozilla with a simple proposal: let's maintain all our tools under one
repo. This would allow us to increase cohesion, share code, maintain a
high quality bar, share best practices, etc. There were no major
objections, so we now have a <a href="https://hg.mozilla.org/hgcustom/version-control-tools/">unified repository</a>
containing our version control tools!</p>
<p>Currently, we only have a few Mercurial extensions in there. A goal is
to accumulate as much of the existing Mercurial infrastructure into
that repository as possible. Client code. Server code. All of the code.
I want developers to be able to install the same hooks on their clients
as what's running on the server: why should your local repo let you
commit something that the server will reject? I want developers to be
able to reasonably reproduce Mozilla's canonical version control server
configuration locally. That way, you can test things locally with a high
confidence that your changes will work the same way on production. This
allows deployments to move faster and with less friction.</p>
<p>The immediate emphasis will be on moving extensions into this repo and
deprecating the old homes on user repositories. Over time, we'll move
into consolidating server code and getting hg.mozilla.org and
git.mozilla.org to use this repository. But that's a lower priority: the
most important goal right now is to make it easier and friendlier for
people to run productivity-enhancing tools.</p>
<p>So, if you see your Mercurial extensions alerting you that they've been
moved to a new repository, now you know what's going on.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[The Mercurial Revlog]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/02/05/the-mercurial-revlog" />
    <id>http://gregoryszorc.com/blog/2014/02/05/the-mercurial-revlog</id>
    <updated>2014-02-05T16:26:00Z</updated>
    <published>2014-02-05T16:26:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[The Mercurial Revlog]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/02/05/the-mercurial-revlog"><![CDATA[<p>Mercurial stores lots of its data in append-only,
intended-to-be-immutable data structures called <em>revlogs</em>. Each revlog is
backed by a file on the filesystem. The main component of a revlog
is a <em>revision</em>. When a new revision arrives, its content is compared
against the previous revision in the file and a zlib-compressed
delta/diff is generated. If the delta is smaller than the entry
itself, the delta is appended to the revlog. If the compressed delta
itself or the stream of deltas that must be applied to recreate the
original text is larger than the source text, the compressed full content
is appended to the revlog. In other words, Mercurial tries to maintain
a balance between file size/growth rate and read times in terms of
both I/O and CPU.</p>
<p>The <em>changelog</em> is a <em>revlog</em> that holds information about every
changeset/commit in the repository. The <em>manifest</em> is a <em>revlog</em> that
holds details about which files are present in every changeset. There
exist a separate <em>revlog</em> for each file/path ever present in the
repository. These are called <em>filelogs</em>.</p>
<p>When you commit a change touching a single file, Mercurial will append
a revision to the changelog describing the commit, a revision to the
manifest describing the set of files active in that specific commit,
and a new revision to a filelog.</p>
<p>You can poke around your repository's revlogs by running some debug
commands. For example:</p>
<pre><code># Show high-level information about the manifest revlog
$ hg debugrevlog -m

# Show details about each entry in the manifest revlog (this
# actually reads data from an index to the data revlog - pretend
# there aren't index files for now)
$ hg debugindex -m

# Dump the content of a single entry in the changelog.
$ hg debugdata -c 2
</code></pre>
<p>I'm generally a big fan of append-only data structures because I love
the beneficial caching properties and linear I/O performance.
Mercurial and the revlog can take advantage of these properties for
some performance wins under key scenarios, especially server
operation (pushing commits doesn't necessarily invalidate cached revlog
entries, allowing the page cache to service many reads).</p>
<h2>Edge cases and deficiencies</h2>
<p>As with any system trying to solve a complex problem, the revlog
storage format doesn't always work as well as intended.</p>
<p>Let's start by taking by looking at how revlogs do delta storage. I said
earlier that revlogs try to store a compressed delta against the
previous entry. Well, that previous entry is the previous physical
revision in the file/revlog, not the parent revision. You see,
entries/revisions in revlogs have a numeric index (0, 1, 2, 3, ...),
a SHA-1 hash of their content, and a link to the logical <em>parent</em> of
the entry. e.g. say you have two commits in your repo, one made after
the other:</p>
<pre><code>0:8ba995b74e18
1:9b2a99adc05e
</code></pre>
<p>The revlog has entries at indexes 0 and 1. They are also accessible
by their hash/node values of 8ba995b74e18 and 9b2a99adc05e. The parent
of 1 is 0. The entry for 0 is the full, compressed content of 0. The
entry for 1 is likely a compressed delta from 0 to 1. As we commit,
we keep appending new entries. These exist as compressed deltas until
the sizes of 0..n is greater than n by itself. At that time, a
compressed version of n is stored.</p>
<p>This model works great for linear histories, as changes from n to n+1
are usually small. Mercurial can store a very small delta for each
entry. The world is good.</p>
<p>This model works great when entries in revlogs are small. By having
small entries (and small changes), the number of deltas required to
eclipse the size of a single entry remains small, in effect keeping the length
of a <em>delta chain</em> in check. Limiting the length of delta chains is good
because it keeps the cost of looking up a single revision's content low.
If you have a delta chain of 1000, for example, Mercurial will need to read
1000 revlog entries and apply 1000 deltas to obtain the original value.
That can get expensive computationally. Since reads are linear, I/O
should remain in check. But you do need to scan a lot of bytes to read
in all the deltas and you need to perform a lot of the same computations
(such as zlib decompression).</p>
<p>Let's talk a little about where the revlog starts to break down.</p>
<p>First, there's the issue of multiple, interleaved branches in the
revlog. Say we have a repository with many branches/heads. We alternate
committing between all the heads. This can play havoc with the default
revlog delta compression. Since revlogs compress against the previous
<em>physical</em> entry in the revlog, if there is lots of alternating between
branches and the contents of those branches diverges significantly, the
deltas can grow quite large and full revisions will be stored with
higher frequency. This means more storage space and more CPU tasked
with resolving deltas (since deltas are larger). Although, CPU is kept
in check since delta chains tend to be smaller since full revisions are
stored with higher frequency.</p>
<p>Second, we have an issue with delta chain explosion of large entries
with small turnover. If your base content is large and it isn't changing
that much, it will take hundreds or even thousands of revisions before
the sum of the delta sizes outgrows that of an entry. This means delta
chains can be very long and Mercurial will have to spend a lot of CPU
to resolve a single entry.</p>
<p>Third, revlogs are using zlib for compression. As many
<a href="http://pokecraft.first-world.info/wiki/Quick_Benchmark:_Gzip_vs_Bzip2_vs_LZMA_vs_XZ_vs_LZ4_vs_LZO">benchmarks</a>
have shown, zlib isn't the fastest or most efficient compression
algorithm in the world. It's likely justified as a reasonable default.
But alternatives exist. The choice of zlib has implications, especially
when other factors (such as excessively long delta chains) come into
play.</p>
<p>Let's talk about mitigation strategies.</p>
<h2>True parent deltas</h2>
<p>While I wasn't around for the original design decisions of revlogs, I'm
guessing they were strongly influenced by the fact that sequential I/O
on magnetic disks is much, much faster than random I/O. With SSDs and
flash storage growing in popularity - a medium that offers random I/O
commonly over 100 MB/s - this buys us the luxury of asking <em>do revlogs
need to be optimized for sequential I/O</em> and <em>how can revlogs change to
take advantage of fast random I/O</em>.</p>
<p>One of the ways revlogs can adapt to fast random I/O is to store the
delta against the logical parent, not the physical. The delta chain will
thus <em>skip</em> revisions in the revlog. e.g. the chain could be 1, 2, 5, 6,
10, not n, n+1, n+2, .... This would keep deltas small and would reduce
the overall size of the revlog. Although, it would likely increase the
length of delta chains, especially when dealing with non-linear
histories.</p>
<p>It turns out Mercurial has a setting that enables this -
<strong>format.generaldelta</strong>. To create a repository with this enabled, run:</p>
<pre><code>$ hg --config format.generaldelta=true init path/to/repo
</code></pre>
<p>The revlogs in that repository with now have deltas computed against the
logical parent!</p>
<p>To verify you are using general delta, look for <strong>generaldelta</strong> in the
<em>.hg/requires</em> file. If it isn't there, you probably don't have
generaldelta enabled. <strong>Please note that cloning a generaldelta repo
won't necessarily give your repo generaldelta. You need to have the
custom config option set on the client or the client will likely use the
defaults.</strong></p>
<p>On repositories with lots of interleaved heads, this can make a huge
difference. As a pathalogical example, Mozilla's
<a href="https://hg.mozilla.org/try/">Try</a> repository (where people push heads
to trigger test/automation runs) has over 22,000 heads. The on-disk
size of the repository is 3117 MB with the standard settings and 2139 MB
with generaldelta! The bulk of this difference comes from the manifest
revlog, which is 954 MB smaller with generaldelta.</p>
<h2>Alternate compression format</h2>
<p>Mercurial uses zlib for revlog compression by default. This is a safe
choice. It's relatively fast and yields relatively good compression.</p>
<p>Since Mercurial is highly extensible, it's possible to plug in a custom
compression format for revlogs. Facebook's
<a href="https://bitbucket.org/facebook/lz4revlog/">lz4revlog extension</a> will
use lz4 for compression. lz4 is faster than zlib, but compression isn't
as good. Repositories with lz4 are commonly ~1.5x larger. But CPU bound
tasks can be significantly faster.</p>
<p>In theory, any compression format can be used. However, it's not
trivially selectable in Mercurial (yet), so someone will need to provide
an extension that implements your desired compression format.</p>
<h2>Alternate storage backends</h2>
<p>In theory, Mercurial revlogs can be backed by anything. This is the
extensibility of Mercurial at work. There's just a Mercurial extension
sitting between using SQL, S3, Cassandra, or any other storage backend
for revlogs.</p>
<p>It's also possible to write custom revlog implementations that change
the file layout for interesting scaling possibilities. For example,
modern filesystems like ZFS and btrfs support block-level deduplication
and transparent compression. If you had block-aligned revlog entries
with deduplication enabled, servers could in theory only store each
revision at most once. Another idea is to let the filesystem handle
the compression. This would cause compression to occur in kernel space
(rather than inside Python userspace). This may have beneficial
performance properties. It may not. It may depend on the repository.
These would be interesting experiments to conduct!</p>
<h2>Caveat with alternate revlog implementations</h2>
<p>Before you go experimenting with alternative revlog implementations,
be forewarned that wall time performance for push and pull operations
may suffer!</p>
<p>Currently, Mercurial isn't as smart as it could be when it comes to
transferring <em>bundles</em> of changeset data between repositories. Let me
explain.</p>
<p>When Mercurial transfers changeset data between repositories, it often
uses an encoding format called a <a href="http://mercurial.selenic.com/wiki/BundleFormat">bundle</a>.
A bundle is effectively a custom archive format for revlog data. If
you've ever used <em>hg bundle</em> or <em>hg unbundle</em> to transfer repository
data, you've explicitly interacted with bundles. But what's lesser known
is that the <em>hg push</em> and <em>hg pull</em> operations also transfer bundles.
The only major difference is that they are created on the fly and their
existence is hidden from view.</p>
<p>In theory, bundles can be encoded a number of different ways. The most
common tunable is the compression format. Over the wire, bundles are
zlib (or even uncompressed). If you run <em>hg bundle</em>, you'll likely
produce a bzip2 bundle. (This is why <em>hg unbundle</em> can be slower than
<em>hg clone</em> - the CPU time spent for bzip2 is much greater than for
zlib.) But a problem with bundles as they exist today is that the format
is rather static when it comes to what's transferred over the wire.
Unless you've mucked about with settings, the client and server will
send a zlib-compressed bundle using the physical revlog order. In other
words, the bundle format is the Mercurial defaults.</p>
<p>If Mercurial were completely dumb, transferring bundles would involve
1) determining the full text of a revlog entry 2) compressing that entry
into a bundle 3) sending that data to a peer 4) decompressing that entry
5) appending that entry to the appropriate revlog (which involves
recompression). Fortunately, Mercurial is a bit smarter than that:
Mercurial can detect when compressed bits in a bundle will match what
is on disk and will avoid the unncessary compression operations.</p>
<p>Anyway, the current bundle transfer mechanism falls apart when there is
a mismatch between client and server configurations or even when the
server or client is using non-defaults. You can think of this as a
revlog impedance mismatch. Essentially, when the client and server are
operating with different or uncommon types of revlogs, Mercurial tends
to default to the lowest common denominator, or zlib deltas against the
physical parent. If, for example, a client wants to use generaldelta
with a server employing defaults, the client will have to convert the
server's deltas into generaldelta deltas. This requires a non-trivial
amount of CPU and pull operations become slower. I believe the opposite
is also true: generaldelta clients will emit generaldelta bundles,
causing the server to recompute the deltas.</p>
<p><strong>When it comes to custom revlog formats today, essentially nobody
wins.</strong></p>
<p>The good news is this will be fixed. There is an
<a href="http://mercurial.selenic.com/wiki/BundleFormat2">effort</a> to improve the
bundle format and wire protocols to make matters better. A little
protocol negotiation can go a long way to make the situation a lot
better. That said, there is still the underlying problem that some
clients may want settings that differ from the server's. e.g. clients
with SSDs likely want generaldelta because they don't need sequential
I/O and SSD space is more expensive, so the smaller repository sizes
achieved with generaldelta are appreciated. But a server operator may
not want to force generaldelta on all clients because it would make
clients on mechanical hard drives slower! The point is revlog impedance
mismatch will occur and someone needs to spend the CPU cycles to rectify
the matter. I suspect this will be pushed to clients since distributed
CPU load is easier to deal with than centralized on the server. But, I
wouldn't be surprised if Mercurial allowed server operators to configure
the behavior. It's a hard problem. Time will tell.</p>
<p>In the mean time, just know that if you experiment with custom revlog
settings, push and pull operations will likely be slower. You may not
notice this on day-to-day operations. But on things like initial clone,
you could experience a massive slow-down.</p>
<p>Here's some timings with
<a href="https://hg.mozilla.org/mozilla-central">mozilla-central</a> with a
Mercurial 1.9 server and client on the same SSD-backed multi-core machine:</p>
<ul>
<li>clone default settings - 4:25</li>
<li>clone --uncompressed - 0:49</li>
<li>clone from generaldelta - 14:11</li>
<li>clone from generaldelta to generaldelta - 16:49</li>
<li>clone from generaldelta --uncompressed - 0:50</li>
<li>pull 2250 changesets, default from default - 6.7s</li>
<li>pull 2250 changesets, default from generaldelta - 17.9s</li>
<li>pull 2250 changesets, generaldelta from default - 28.5s</li>
<li>pull 2250 changesets, generaldelta from generaldelta - 20.0s</li>
</ul>
<p>As you can see, anything involving compression and generaldelta is
much slower. Once <em>bundle format 2</em> is fully implemented, I expect the
situation to improve.  Until then, know that you'll get a ~2.5-4x
slowdown from using generaldelta. You'll have to measure other
revlog formats for their impact.</p>
<p>In conclusion, Mercurial's revlog file format is an interesting and
tunable data structure. It works pretty well for most repositories. But
if you are the 1%, you might want to spend some time to investigate
changing its default configuration.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Review Board at Mozilla]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/01/27/review-board-at-mozilla" />
    <id>http://gregoryszorc.com/blog/2014/01/27/review-board-at-mozilla</id>
    <updated>2014-01-27T16:30:00Z</updated>
    <published>2014-01-27T16:30:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <category scheme="http://gregoryszorc.com/blog" term="mach" />
    <summary type="html"><![CDATA[Review Board at Mozilla]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/01/27/review-board-at-mozilla"><![CDATA[<p>Some Mozillians recently stood up an
<a href="https://reviewboard.allizom.org/">instance</a> of
<a href="http://www.reviewboard.org/">Review Board</a> - a web-based code review
tool for evaluation purposes. Having used Review Board at a previous
company, I can say with high confidence that when properly configured,
it beats the pants off Splinter (the code review interface baked into
Bugzilla). Here are some advantages:</p>
<ul>
<li>The HTML interface is more pleasant on the eyes (subjective).</li>
<li>Interdiffs actually work.</li>
<li>Intra-line diffs are rendered.</li>
<li>You can open <em>issues</em> for individual review comments and these
  issues can be tracked during subsequent reviews (Bugzilla doesn't
  really have anything similar and review comments tend to get lost
  unless the reviewer is sharp).</li>
<li>It integrates with the VCS, so you can expand code context from the
  review interface.</li>
<li>There are buttons to toggle whitespace differences.</li>
<li>Syntax hightlighting! It even recognizes things like <em>TODO</em> in
  comments.</li>
</ul>
<p>You can read more from the <a href="http://www.reviewboard.org/docs/manual/1.7/users/">official user guide</a>.</p>
<p>If you have any interest in evaluating Review Board, the easiest way
to upload patches to Mozilla's instance is to run <strong>mach rbt</strong>.</p>
<p><strong>mach rbt</strong> will launch the Review Board tools command-line client
(called RBTools). From there, you can do a number of things. Run
<strong>mach rbt help</strong> to see the full list.</p>
<p>Here are some examples:</p>
<pre><code># See a diff that would be uploaded to Review Board:
$ mach rbt diff

# Create a review request based on the current Mercurial changeset:
$ mach rbt post

# That should print out a URL to the not-yet-published review
# request. If you go to that URL, you'll notice that the fields
# in that request are all empty.

# Next time, you can have some fields auto-populate by running:
$ mach rbt post --guess-summary --guess-description

# This grabs info from the commit message.

# To update an existing review request (e.g. to submit a new patch):
$ mach rbt post -r &lt;review id&gt;

# (where &lt;review ID&gt; is the ID of the review).

# You can also have it generate a "squashed" patch from multiple
# commits:
$ mach rbt post 164123::164125
</code></pre>
<p>Run <strong>mach rbt help post</strong> for more options. Also see the
<a href="http://www.reviewboard.org/docs/rbtools/dev/">RBTools documentation</a>
for more.</p>
<p>It's worth noting that <strong>mach rbt</strong> will download an unreleased version
of RBTools. This is because the released version doesn't work well with
Mercurial. I contributed a handful of patches to RBTools to make
Mercurial work better.</p>
<p>Before you dive in and start using Review Board for actual code review,
there are some things you need to know:</p>
<ul>
<li>Mozilla's Review Board instance does not yet send emails on changes.
  <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=958236">Bug 958236</a>
  tracks this. When it works, you'll see nice emails, just like you do
  for Bugzilla reviews.</li>
<li>Review Board doesn't currently interact with Bugzilla that well. In
  theory, we could have Review Board update corresponding Bugzilla bugs
  when actions are performed. Someone just needs to write this code
  and get it deployed.</li>
<li>If you create a Bugzilla attachment that contains the URL of a Review
  Board review (e.g. https://reviewboard.allizom.org/r/23/), Bugzilla
  will automatically set the MIME type as a Review Board review and set
  up an HTML redirect when the attachment is <em>downloaded</em> via the
  browser. You can even set <strong>r?</strong> on this attachment to have Bugzilla
  nag about reviews. See <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=875562">bug 875562</a>
  for an example.</li>
<li>There is currently no way to upload a patch to Review Board and update
  Bugzilla is one go. I have proof-of-concept code for this. Although,
  there is <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=947596">pushback</a>
  on getting that checked in.</li>
<li>Review Board 2 is in development. It has a number of new and exciting
  features. And it looks better.</li>
</ul>
<p>Finally and most importantly, <strong>Review Board at Mozilla is still in
evaluation mode.</strong> It's usage has not been officially blessed as far
as I know. I don't believe the SLA is as high as other services (like
Bugzilla). Nobody is really using it yet. It still needs a lot of
polish and integration for it to realize its potential. And, there is
some <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=947596">talk</a>
about the future of code review at Mozilla that may or may not involve
Review Board. In short, the future of Review Board at Mozilla is
uncertain. I wouldn't rely on it to archive review comments from super
important reviews/decisions.</p>
<p>Despite the shortcomings, I encourage people to play around with Review
Board. If nothing else, at least <a href="https://reviewboard.allizom.org/r/24/diff/">gaze upon it's patch rendering
beauty</a> and witness
what the future could hold.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Aggregating Version Control Info at Mozilla]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/01/21/aggregating-version-control-info-at-mozilla" />
    <id>http://gregoryszorc.com/blog/2014/01/21/aggregating-version-control-info-at-mozilla</id>
    <updated>2014-01-21T10:50:00Z</updated>
    <published>2014-01-21T10:50:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Git" />
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <category scheme="http://gregoryszorc.com/blog" term="Python" />
    <summary type="html"><![CDATA[Aggregating Version Control Info at Mozilla]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/01/21/aggregating-version-control-info-at-mozilla"><![CDATA[<p>Over the winter break, I set out on an ambitious project to create
a service to help developers and others manage the flury
of patches going into Firefox. While the project is far from complete,
I'm ready to unleash the first part of the project upon the world.</p>
<p>If you point your browsers to
<a href="http://moztree.gregoryszorc.com/">moztree.gregoryszorc.com</a>, you'll
hopefully see some documentation about what I've built.
<a href="https://bitbucket.org/indygreg/moz-tree-utopia">Source code</a> is
available and free, of course. Patches very welcome.</p>
<p>Essentially, I built a centralized indexing service for version
control repositories with Mozilla's extra metadata thrown in.
I tell it what repositories to mirror, and it clones everything,
fetches data such as the pushlog and Git SHA-1 mappings, and
stores everything in a central database. It then exposes this
aggregated data through world-readable web services.</p>
<p>Currently, I have the service indexing the popular project branches
for Firefox (central, aurora, beta, release, esr, b2g, inbound, fx-team,
try, etc). You can view the
<a href="http://moztree.gregoryszorc.com/api/repos">full list</a> via the web
service. As a bonus, I'm also serving these repositories via
<a href="http://hg.gregoryszorc.com/">hg.gregoryszorc.com</a>. My server appears
to be significantly faster than
<a href="https://hg.mozilla.org">hg.mozilla.org</a>. If you want to use it for
your daily needs, go for it. I make no SLA guarantees, however.</p>
<p>I'm also using this service as an opportunity to experiment with
alternate forms of Mercurial hosting. I have mirrors of mozilla-central
and the try repository with generaldelta and lz4 compression enabled.
I may blog about what those are eventually. The teaser is that they can
make Mercurial perform a lot faster under some conditions. I'm also
using ZFS under the hood to manage repositories. Each repository is a
ZFS filesystem. This means I can create repository copies on the server
(user repositories anyone?) at a nearly free cost. Contrast this to the
traditional method of full clones, which take lots of time, memory, CPU,
and storage.</p>
<p>Anyway, some things you can do with the existing web service:</p>
<ul>
<li>Obtain metadata about Mercurial changesets.
  <a href="http://moztree.gregoryszorc.com/api/changeset/940b2974f35b">Example</a>.</li>
<li>Look up metadata about Git commits.
  <a href="http://moztree.gregoryszorc.com/api/git-sha1/40438af67c321">Example</a>.</li>
<li>Obtain a <a href="http://moztree.gregoryszorc.com/api/spore">SPORE descriptor</a>
  describing the web service endpoints. This allows you to auto-generate
  clients from descriptors. Yay!</li>
</ul>
<p>Obviously, that's not a lot. But adding new endpoints is relatively
straightforward. See the <a href="https://bitbucket.org/indygreg/moz-tree-utopia/src/tip/repodata/web/app.py">source</a>.
It's literally as easy as defining a URL mapping and writing a
database query.</p>
<p>The performance is also not the best. I just haven't put in the effort
to tune things yet. All of the querying hits the database, not
Mercurial. So, making things faster should merely be a matter of
database and hosting optimization. Patches welcome!</p>
<p>Some ideas that I haven't had time to implement yet:</p>
<ul>
<li>Return changests in a specific repository</li>
<li>Return recently pushed changesets</li>
<li>Return pushes for a given user</li>
<li>Return commits for a given author</li>
<li>Return commits referencing a given bug</li>
<li>Obtain TBPL URLs for pushes with changeset</li>
<li>Integrate bugzilla metadata</li>
</ul>
<p>Once those are in place, I foresee this service powering a number of
dashboards. Patches welcome.</p>
<p>Again, this service is only the tip of what's possible. There's a lot
that could be built on this service. I have ideas. Others have ideas.</p>
<p>The project includes a Vagrant file and Puppet
manifests for provisioning the server. It's a one-liner to get a
development environment up and running. It should be really easy to
contribute to this project. Patches welcome.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Things Mozilla Could Do with Mercurial]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/01/17/things-mozilla-could-do-with-mercurial" />
    <id>http://gregoryszorc.com/blog/2014/01/17/things-mozilla-could-do-with-mercurial</id>
    <updated>2014-01-17T15:00:00Z</updated>
    <published>2014-01-17T15:00:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mercurial" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Things Mozilla Could Do with Mercurial]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/01/17/things-mozilla-could-do-with-mercurial"><![CDATA[<p>As I've <a href="/blog/category/mercurial/">written before</a>, Mercurial is a
highly extensible version control system. You can do things with
Mercurial you can't do in other version control systems.</p>
<p>In this post, I'll outline some of the cool things Mozilla could do with
Mercurial. But first, I want to outline some features of Mercurial that
many don't know exist.</p>
<h2>pushkey and listkeys command</h2>
<p>The Mercurial wire protocol (how two Mercurial peer repositories
talk to each other over a network) contains two very useful commands:
<em>pushkey</em> and <em>listkeys</em>. These commands allow the storage and listing
of arbitrary key-value pair metadata in the repository.</p>
<p>This generic storage mechanism is how Mercurial stores and synchronizes
bookmarks and phases information, for example.</p>
<p>By implementing a Mercurial extension, you can have Mercurial store
key-value data for any arbitrary data namespace. You can then write
a simple extension that synchronizes this data as part of the push
and pull operations.</p>
<h2>Extending the wire protocol</h2>
<p>For cases where you want to transmit arbitrary data to/from Mercurial
servers and where the <em>pushkey</em> framework isn't robust enough, it's
possible to implement custom commands in the Mercurial wire protocol.</p>
<p>A server installs an extension making the commands available. A client
installs an extension knowing how to use the commands. Arbitrary data
is transferred or custom actions are performed.</p>
<p>When it comes to custom commands, the sky is really the limit. You
could do pretty much anything from transfer extra data types (this
is how the <a href="http://mercurial.selenic.com/wiki/LargefilesExtension">largefiles extension</a>
works) to writing commands that interact with remote agents.</p>
<h2>Custom revision set queries and templating</h2>
<p>Mercurial offers a rich framework for querying repository data and
for formatting data. The querying is called <em>revision sets</em> and the
later <em>templates</em>. If you are unfamiliar with the feature, I
encourage you to run <em>hg help revset</em> and <em>hg help templates</em> right
now to discover the awesomeness.</p>
<p>As I've <a href="/blog/2013/11/08/using-mercurial-to-query-mozilla-metadata/">demonstrated</a>,
you can do some very nifty things with custom revision sets and
templating!</p>
<h2>The possibilities</h2>
<p>Now that you know some ways Mercurial can be extended, let's talk about
some cool use cases at Mozilla. I want to be clear that I'm not
advocating we do these things, just that they are possible and maybe
they are a little cool.</p>
<h3>Storing pushlog data</h3>
<p>Mozilla records information about who pushed what changesets where and
when in what's called the <em>pushlog</em>. The pushlog data is currently
stored in a SQLite database inside the repository on the server. The
data is made available via a HTTP+JSON API.</p>
<p>We could go a step further and make the pushlog data available via
<em>listkeys</em> so Mercurial clients could download pushlog data with the
same channel used to pull core repository data. (Currently, we have
to open a new TCP connection and talk to the HTTP+JSON API.) This
would make fetching of pushlog data faster, especially for clients
on slow connections.</p>
<p>I concede this is an iterative improvement and adds little value beyond
what we currently have. But if I were designing pushlog storage from
scratch, this is how I'd do it.</p>
<h3>Storing a changeset's automation results</h3>
<p>The <em>pushkey</em> framework could be used to mark specific changesets
as passing automation. When release automation or a sheriff determines
that a changeset/push is green, they could issue an authenticated
<em>pushkey</em> command to the Mercurial server stating such. Clients
could then easily obtain a list of all changesets that are green.</p>
<p>Why stop there? We could also record automation failures in Mercurial as
well. Depending on how complex this gets, we may outgrow <em>pushkey</em>
and require a separate command. But that's all doable.</p>
<p>Anyway, clients could download automation results for a specific
changeset as part of the repository data. The same extension that
pulls down that data could also monkeypatch the bisection algorithm used
by <em>hg bisect</em> to automatically skip over changesets that didn't pass
automation. You'll never bisect a backed out changeset again!</p>
<p>If this automation data were stored on the Try repository, the autoland
tool would just need to query the Mercurial repo to see which changesets
are candidates for merging into mainline - there would be no need for
a separate database and web service!</p>
<h3>Marking a changeset as reviewed</h3>
<p>Currently, Mozilla's review procedure is very patch and Bugzilla
centric. But it doesn't have to be that way. (I argue it shouldn't be
that way.)</p>
<p>Imagine a world where code review is initiated by pushing changesets
to a special server, kind of like how Try magically turns pushes into
automation jobs.</p>
<p>In this world, reviews could be initiated by issuing a <em>pushkey</em>
or custom command to the server. This could even initiate
server-side static analysis that would hold off publishing the review
unless static analysis checks passed!</p>
<p>Granted review could be recorded by having someone issue a
<em>pushkey</em> command to mark a changeset as reviewed. The channel to the
Mercurial server is authenticated via SSH, so the user behind the
current SSH key is the reviewer. The Mercurial server could store this
username as part of the repository data. The autoland tool could then
pull down the reviewer data and only consider changesets that have an
appropriate reviewer.</p>
<p>It <em>might</em> also be possible to integrate crypto magic into this
workflow so reviewers could digitally sign a changeset as reviewed.
This could help with the verification of the Firefox source code
that Brendan Eich <a href="https://brendaneich.com/2014/01/trust-but-verify/">recently outlined</a>.</p>
<p>Like the automation data above, no separate
database would be required: all data would be part of the repository.
All you need to build is a Mercurial extension.</p>
<h3>Encouraging best practices</h3>
<p>Mozillians have written a handful of useful Mercurial extensions to
help people become more productive. We have also noticed that many
developers are still (unknowingly?) running old, slow, and buggy
Mercurial releases. We want people to have the best experience possible.
How do we do that?</p>
<p>One idea is to install an extension on the server that strongly
recommands or even requires users follow best practices (minimal HG
version, installed extensions, etc).</p>
<p>I have developed a <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=941856">proof-of-concept</a>
that does just this.</p>
<h3>Rich querying of metadata</h3>
<p>When you start putting more metadata into Mercurial (or at least write
Mercurial extensions to aggregate this metadata), all kinds of
interesting query opportunities open up. Using revsets and templates,
you can do an awful lot to use Mercurial as a database of sorts
to extract useful reports.</p>
<p>I dare say reports like
<a href="http://oduinn.com/blog/2013/12/03/infrastructure-load-for-november-2013/">John O'duinn's Monthly Infrastructure Load</a>
posts could be completely derived from Mercurial. I've
<a href="/blog/2013/11/08/using-mercurial-to-query-mozilla-metadata/">demonstrated</a>
this ability previously. That's only the tip of the iceburg.</p>
<h2>Summary</h2>
<p>We could enable a lot of new and useful scenarios by extending
Mercurial. We could accomplish this without introducing new
services and tools into our already complicated infrastructure
and workflows.</p>
<p>The possibilities I've suggested are by no means exhaustive. I encourage
others to dream up new and interesting ideas. Who knows, maybe some of
them may actually happen.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[mach now lives in mozilla-central]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/01/09/mach-now-lives-in-mozilla-central" />
    <id>http://gregoryszorc.com/blog/2014/01/09/mach-now-lives-in-mozilla-central</id>
    <updated>2014-01-09T10:55:00Z</updated>
    <published>2014-01-09T10:55:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <category scheme="http://gregoryszorc.com/blog" term="mach" />
    <summary type="html"><![CDATA[mach now lives in mozilla-central]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/01/09/mach-now-lives-in-mozilla-central"><![CDATA[<p><a href="https://pypi.python.org/pypi/mach/">mach</a> -- the generic command
line interface framework that is behind the <em>mach</em> tool used to
build Firefox -- now has its canonical home in
<a href="https://hg.mozilla.org/mozilla-central/file/default/python/mach/">mozilla-central</a>,
the canonical repository for Firefox. The
<a href="https://github.com/indygreg/mach">previous home</a> has been updated to
reflect the change.</p>
<p>mach will continue to be released on
<a href="https://pypi.python.org/pypi/mach/">PyPI</a> and installable via <strong>pip
install mach</strong>.</p>
<p>I made the change because keeping multiple repositories in sync
wasn't something I wanted to spend time doing. Furthermore,
Mozillians have been contributing a steady stream of improvements to
the mach core recently and it makes sense to leverage Mozilla's
familiar infrastructure for patch contribution.</p>
<p>This decision may be revisited in the future. Time will tell.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[Why do Projects Support old Python Releases]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/01/08/why-do-projects-support-old-python-releases" />
    <id>http://gregoryszorc.com/blog/2014/01/08/why-do-projects-support-old-python-releases</id>
    <updated>2014-01-09T16:05:00Z</updated>
    <published>2014-01-08T17:00:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Python" />
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[Why do Projects Support old Python Releases]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/01/08/why-do-projects-support-old-python-releases"><![CDATA[<p>I see a number of open source projects supporting old versions
of Python. Mercurial supports 2.4, for example. I have to ask: why do
projects continue to support old Python releases?</p>
<p>Consider:</p>
<ul>
<li><a href="http://python.org/download/releases/2.4.6/">Python 2.4</a> was last
  released on December 19, 2008 and <strong>there will be no more releases of
  Python 2.4</strong>.</li>
<li><a href="http://python.org/download/releases/2.5.6/">Python 2.5</a> was last
  released on May 26, 2011 and <strong>there will be no more releases of
  Python 2.5</strong>.</li>
<li><a href="http://python.org/download/releases/2.6.9/">Python 2.6</a> was last
  released on October 29, 2013 and <strong>there will be no more releases of
  Python 2.6</strong>.</li>
<li><strong>Everything before Python 2.7 is end-of-lifed</strong></li>
<li>Python 2.7 continues to see periodic releases, but mostly for bug fixes.</li>
<li>Practically all of the work on CPython is happening in the 3.3 and 3.4
  branches. Other implementations continue to support 2.7.</li>
<li>Python 2.7 has been available since July 2010</li>
<li>Python 2.7 provides some very compelling language features over
  earlier releases that developers want to use</li>
<li>It's much easier to write dual compatible 2/3 Python when 2.7 is the
  only 2.x release considered.</li>
<li>Python 2.7 can be installed in userland relatively easily (see
  projects like <a href="https://github.com/yyuu/pyenv">pyenv</a>).</li>
</ul>
<p>Given these facts, I'm not sure why projects insist on supporting old
and end-of-lifed Python releases.</p>
<p><strong>I think maintainers of Python projects should seriously consider
dropping support for Python 2.6 and below.</strong> Are there really that many
people on systems that don't have Python 2.7 easily available? Why are
we Python developers inflicting so much pain on ourselves to support
antiquated Python releases?</p>
<p>As a data point, I successfully transitioned Firefox's build system
from requiring Python 2.5+ to 2.7.3+ and it was relatively
<a href="https://groups.google.com/d/msg/mozilla.dev.platform/djN02O03APc/OS8A9LuHX0sJ">pain</a>
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=870420">free</a>.
Sure, a few people complained. But as far as I know, not very many new
developers are coming in and complaining about the requirement.
If we can do it with a few thousand developers, I'm guessing your
project can as well.</p>
<p><strong>Update 2014-01-09 16:05:00 PST</strong>: This post is being discussed on
<a href="http://developers.slashdot.org/story/14/01/09/1940232/why-do-projects-continue-to-support-old-python-releases">Slashdot</a>. A lot of the comments
talk about Python 3. Python 3 is its own set of
considerations. The intended focus of this post is strictly about
dropping support for Python 2.6 and below. Python 3 is related
in that porting Python 2.x to Python 3 is much easier the higher
the Python 2.x version. This especially holds true when you want
to write Python that works simultaneously in both 2.x and 3.x.</p>]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://gregoryszorc.com/blog</uri>
    </author>
    <title type="html"><![CDATA[On Multiple Patches in Bugs]]></title>
    <link rel="alternate" type="text/html" href="http://gregoryszorc.com/blog/2014/01/07/on-multiple-patches-in-bugs" />
    <id>http://gregoryszorc.com/blog/2014/01/07/on-multiple-patches-in-bugs</id>
    <updated>2014-01-07T16:40:00Z</updated>
    <published>2014-01-07T16:40:00Z</published>
    <category scheme="http://gregoryszorc.com/blog" term="Mozilla" />
    <summary type="html"><![CDATA[On Multiple Patches in Bugs]]></summary>
    <content type="html" xml:base="http://gregoryszorc.com/blog/2014/01/07/on-multiple-patches-in-bugs"><![CDATA[<p>There is a common practice at Mozilla for developing patches with
multiple parts. Nothing wrong with that. In fact, I think it's a best
practice:</p>
<ul>
<li>Smaller, self-contained patches are much easier to grok and
  review than larger patches.</li>
<li>Smaller patches can land as soon as they are reviewed. Larger patches
  tend to linger and get bit rotted.</li>
<li>Smaller patches contribute to a culture of being fast and nimble, not
  slow and lethargic. This helps with developer confidence, community
  contributions, etc.</li>
</ul>
<p>There are some downsides to multiple, smaller patches:</p>
<ul>
<li>The bigger picture is harder to understand until all parts of a
  logical patch series are shared. (This can be alleviated through
  commit messages or reviewer notes documenting future intentions.
  And of course reviewers can delay review until they are comfortable.)</li>
<li>There is more overhead to maintain the patches (rebasing, etc).
  IMO the solutions provided by Mercurial and Git are sufficient.</li>
<li>The process overhead for dealing with multiple patches and/or bugs
  can be non-trivial. (I would like to think good tooling coupled with
  continuous revisiting of policy decisions is sufficient to counteract
  this.)</li>
</ul>
<p>Anyway, the prevailing practice at Mozilla seems to be that multiple
patches related to the same logical change are attached to the same
bug. I would like to challenge the effectiveness of this practice.</p>
<p>Given:</p>
<ul>
<li>An individual commit to Firefox should be standalone and should not
  rely on future commits to unbust it (i.e. bisects to any commit should
  be safe).</li>
<li>Bugzilla has no good mechanism to isolate review comments from
  multiple attachments on the same bug, making deciphering simultaneous
  reviews on multiple attachments difficult and frustrating. This leads
  to review comments inevitably falling through the cracks and the
  quality of code suffering.</li>
<li>Reiterating the last point because it's important.</li>
</ul>
<p>I therefore argue that attaching multiple reviews to a single Bugzilla
bug is not a best practice and it should be avoided if possible. If that
means filing separate bugs for each patch, so be it. That process can
be automated. Tools like
<a href="https://hg.mozilla.org/users/tmielczarek_mozilla.com/bzexport">bzexport</a>
already do it. Alternatively (and even better IMO), we ditch
Bugzilla's code review interface (Splinter) and integrate something like
<a href="https://reviewboard.allizom.org/">ReviewBoard</a> instead. We limit
Bugzilla to tracking, high-level discussion, and metadata aggregation.
Code review happens elsewhere, without all the clutter and chaos that
Bugzilla brings to the table.</p>
<p>Thoughts?</p>]]></content>
  </entry>
</feed>
